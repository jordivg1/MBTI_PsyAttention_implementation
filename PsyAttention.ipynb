{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bedd3c7141445e2bfa4d8e48ea9f816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d00b32be718a4089a04fef7edf96a0f3",
              "IPY_MODEL_4a7f3db2e1114145b9ba0c650d04e072",
              "IPY_MODEL_82b05cc6a51d43c581f11f815ef3d707"
            ],
            "layout": "IPY_MODEL_028e2404741c459bb74d41940c8c212e"
          }
        },
        "d00b32be718a4089a04fef7edf96a0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6d4e5bf9a39433fbac08be1484f123d",
            "placeholder": "​",
            "style": "IPY_MODEL_3ad06ecaa5394a71b015b3482fc3c497",
            "value": "Epoch 1/10: 100%"
          }
        },
        "4a7f3db2e1114145b9ba0c650d04e072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a82cfb2bd974e258a6d1d3ff26b4937",
            "max": 87,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c02447d17afc43a9b5ae844f6056f9b2",
            "value": 87
          }
        },
        "82b05cc6a51d43c581f11f815ef3d707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3067040960c24584a2a82922747694f3",
            "placeholder": "​",
            "style": "IPY_MODEL_360aa61a1392447b9268c1dc6000eed3",
            "value": " 87/87 [09:16&lt;00:00,  5.90s/it, loss=0.742]"
          }
        },
        "028e2404741c459bb74d41940c8c212e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6d4e5bf9a39433fbac08be1484f123d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad06ecaa5394a71b015b3482fc3c497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a82cfb2bd974e258a6d1d3ff26b4937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02447d17afc43a9b5ae844f6056f9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3067040960c24584a2a82922747694f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "360aa61a1392447b9268c1dc6000eed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb8978a3b5274404b6638dfeb5c395a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39300b29da604252b6257c6d7a972af7",
              "IPY_MODEL_cc00e001a3cd4d6e8cd24e40681c54ed",
              "IPY_MODEL_0372e3d75e5d4094a1fd4a941125de23"
            ],
            "layout": "IPY_MODEL_26169114a1394f66896aa06b2972c1ad"
          }
        },
        "39300b29da604252b6257c6d7a972af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3affcffc8b64b928c0ddfe0d5475256",
            "placeholder": "​",
            "style": "IPY_MODEL_0f5f663ee9bb499491342eb602d2e43a",
            "value": "Epoch 2/10:   0%"
          }
        },
        "cc00e001a3cd4d6e8cd24e40681c54ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2444232434d4061ac33e2825e92d361",
            "max": 87,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c42333cecd94a959a3d425854076cc2",
            "value": 0
          }
        },
        "0372e3d75e5d4094a1fd4a941125de23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1de6e9db7c864e4fa4cf5403073650f5",
            "placeholder": "​",
            "style": "IPY_MODEL_0c76ce31924f43f8b73a5d0eab1c70d5",
            "value": " 0/87 [00:00&lt;?, ?it/s]"
          }
        },
        "26169114a1394f66896aa06b2972c1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3affcffc8b64b928c0ddfe0d5475256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5f663ee9bb499491342eb602d2e43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2444232434d4061ac33e2825e92d361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c42333cecd94a959a3d425854076cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1de6e9db7c864e4fa4cf5403073650f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c76ce31924f43f8b73a5d0eab1c70d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INFO SOBRE LOS .CSV**\n"
      ],
      "metadata": {
        "id": "sJQYjjNKYvbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m0GqBzgYYuPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jgPK6N0sXvSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a944cce4-a5bc-4faa-9347-8109c93445b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "file_path = '/content/drive/MyDrive/mbti_1_selected_features.csv'\n",
        "\n",
        "mbti_data = pd.read_csv(file_path).drop(['mbti_type', 'Filename', 'file_index'], axis=1)\n"
      ],
      "metadata": {
        "id": "YkTsEez6L38B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(mbti_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "id": "o0ePRN4OMSne",
        "outputId": "41502e8f-3c04-479e-b603-f23b99a041a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      Arousal_nwords  Comnobj_GI   Fall_GI  Compassion_GALC   Dist_GI  \\\n",
              "0           0.373525    0.007194  0.003597         0.000000  0.000000   \n",
              "1           0.320863    0.003419  0.000000         0.000000  0.000855   \n",
              "2           0.359031    0.009569  0.000000         0.001196  0.000000   \n",
              "3           0.272979    0.001880  0.000940         0.000000  0.001880   \n",
              "4           0.332802    0.002068  0.000000         0.000000  0.000000   \n",
              "...              ...         ...       ...              ...       ...   \n",
              "8670        0.468492    0.002513  0.000000         0.000000  0.000000   \n",
              "8671        0.311636    0.007645  0.000765         0.000000  0.000000   \n",
              "8672        0.394430    0.004219  0.000000         0.001055  0.000000   \n",
              "8673        0.310370    0.013490  0.001173         0.000587  0.000000   \n",
              "8674        0.374532    0.004422  0.000737         0.000000  0.001474   \n",
              "\n",
              "      Freq_N_OG_FW  Active_GI   Stay_GI    Dim_GI  Contempt_GALC  ...  \\\n",
              "0        10.580120   0.091727  0.005396  0.007194        0.00000  ...   \n",
              "1        10.765889   0.139316  0.001709  0.006838        0.00000  ...   \n",
              "2        10.684414   0.108852  0.003589  0.001196        0.00000  ...   \n",
              "3        10.679634   0.121241  0.000940  0.004699        0.00000  ...   \n",
              "4        10.379383   0.118925  0.002068  0.004137        0.00000  ...   \n",
              "...            ...        ...       ...       ...            ...  ...   \n",
              "8670     10.835137   0.110553  0.000000  0.000000        0.00000  ...   \n",
              "8671     10.911671   0.129205  0.001529  0.002294        0.00000  ...   \n",
              "8672     10.487508   0.117089  0.003165  0.002110        0.00000  ...   \n",
              "8673     11.016429   0.114956  0.002933  0.005279        0.00000  ...   \n",
              "8674     10.905752   0.125276  0.003685  0.004422        0.00147  ...   \n",
              "\n",
              "      Causal_GI  Boredom_GALC  Joy_GALC  lexical_density_types  eat_types_CW  \\\n",
              "0      0.017986      0.000000  0.000000               0.779292     40.541502   \n",
              "1      0.019658      0.001709  0.000000               0.809426     40.285714   \n",
              "2      0.026316      0.001196  0.000000               0.790361     39.626478   \n",
              "3      0.031015      0.000000  0.000000               0.777056     41.276087   \n",
              "4      0.019648      0.000000  0.000000               0.787472     40.484144   \n",
              "...         ...           ...       ...                    ...           ...   \n",
              "8670   0.032663      0.000000  0.000000               0.801609     42.087193   \n",
              "8671   0.022936      0.001528  0.000764               0.791753     40.250420   \n",
              "8672   0.018987      0.000000  0.000000               0.779736     39.414414   \n",
              "8673   0.028152      0.000000  0.000000               0.821818     40.010243   \n",
              "8674   0.019160      0.000000  0.000000               0.792035     39.641813   \n",
              "\n",
              "      E/I  S/N  T/F  J/P                                              posts  \n",
              "0       0    0    0    1  http://www.youtube.com/watch?v=qsXHcwe3krw|||h...  \n",
              "1       1    0    1    0  I'm finding the lack of me in these posts very...  \n",
              "2       0    0    1    0  Good one  _____   https://www.youtube.com/watc...  \n",
              "3       0    0    1    1  Dear INTP,   I enjoyed our conversation the ot...  \n",
              "4       1    0    1    1  You're fired.|||That's another silly misconcep...  \n",
              "...   ...  ...  ...  ...                                                ...  \n",
              "8670    0    1    0    0  https://www.youtube.com/watch?v=t8edHB_h908|||...  \n",
              "8671    1    0    0    0  So...if this thread already exists someplace e...  \n",
              "8672    0    0    1    0  So many questions when i do these things.  I w...  \n",
              "8673    0    0    0    0  I am very conflicted right now when it comes t...  \n",
              "8674    0    0    0    0  It has been too long since I have been on pers...  \n",
              "\n",
              "[8675 rows x 65 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-082bed91-b151-4b91-8fa5-a14117486725\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Arousal_nwords</th>\n",
              "      <th>Comnobj_GI</th>\n",
              "      <th>Fall_GI</th>\n",
              "      <th>Compassion_GALC</th>\n",
              "      <th>Dist_GI</th>\n",
              "      <th>Freq_N_OG_FW</th>\n",
              "      <th>Active_GI</th>\n",
              "      <th>Stay_GI</th>\n",
              "      <th>Dim_GI</th>\n",
              "      <th>Contempt_GALC</th>\n",
              "      <th>...</th>\n",
              "      <th>Causal_GI</th>\n",
              "      <th>Boredom_GALC</th>\n",
              "      <th>Joy_GALC</th>\n",
              "      <th>lexical_density_types</th>\n",
              "      <th>eat_types_CW</th>\n",
              "      <th>E/I</th>\n",
              "      <th>S/N</th>\n",
              "      <th>T/F</th>\n",
              "      <th>J/P</th>\n",
              "      <th>posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.373525</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.580120</td>\n",
              "      <td>0.091727</td>\n",
              "      <td>0.005396</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017986</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.779292</td>\n",
              "      <td>40.541502</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>http://www.youtube.com/watch?v=qsXHcwe3krw|||h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.320863</td>\n",
              "      <td>0.003419</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000855</td>\n",
              "      <td>10.765889</td>\n",
              "      <td>0.139316</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>0.006838</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019658</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.809426</td>\n",
              "      <td>40.285714</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I'm finding the lack of me in these posts very...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.359031</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.684414</td>\n",
              "      <td>0.108852</td>\n",
              "      <td>0.003589</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.790361</td>\n",
              "      <td>39.626478</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Good one  _____   https://www.youtube.com/watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.272979</td>\n",
              "      <td>0.001880</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001880</td>\n",
              "      <td>10.679634</td>\n",
              "      <td>0.121241</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.777056</td>\n",
              "      <td>41.276087</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear INTP,   I enjoyed our conversation the ot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.332802</td>\n",
              "      <td>0.002068</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.379383</td>\n",
              "      <td>0.118925</td>\n",
              "      <td>0.002068</td>\n",
              "      <td>0.004137</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019648</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.787472</td>\n",
              "      <td>40.484144</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>You're fired.|||That's another silly misconcep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8670</th>\n",
              "      <td>0.468492</td>\n",
              "      <td>0.002513</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.835137</td>\n",
              "      <td>0.110553</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.801609</td>\n",
              "      <td>42.087193</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.youtube.com/watch?v=t8edHB_h908|||...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8671</th>\n",
              "      <td>0.311636</td>\n",
              "      <td>0.007645</td>\n",
              "      <td>0.000765</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.911671</td>\n",
              "      <td>0.129205</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.002294</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022936</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.791753</td>\n",
              "      <td>40.250420</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>So...if this thread already exists someplace e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8672</th>\n",
              "      <td>0.394430</td>\n",
              "      <td>0.004219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.487508</td>\n",
              "      <td>0.117089</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018987</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.779736</td>\n",
              "      <td>39.414414</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>So many questions when i do these things.  I w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8673</th>\n",
              "      <td>0.310370</td>\n",
              "      <td>0.013490</td>\n",
              "      <td>0.001173</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.016429</td>\n",
              "      <td>0.114956</td>\n",
              "      <td>0.002933</td>\n",
              "      <td>0.005279</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028152</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.821818</td>\n",
              "      <td>40.010243</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I am very conflicted right now when it comes t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8674</th>\n",
              "      <td>0.374532</td>\n",
              "      <td>0.004422</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001474</td>\n",
              "      <td>10.905752</td>\n",
              "      <td>0.125276</td>\n",
              "      <td>0.003685</td>\n",
              "      <td>0.004422</td>\n",
              "      <td>0.00147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.792035</td>\n",
              "      <td>39.641813</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>It has been too long since I have been on pers...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8675 rows × 65 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-082bed91-b151-4b91-8fa5-a14117486725')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-082bed91-b151-4b91-8fa5-a14117486725 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-082bed91-b151-4b91-8fa5-a14117486725');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c74d1030-662b-4249-aa0f-77c8abe393b9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c74d1030-662b-4249-aa0f-77c8abe393b9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c74d1030-662b-4249-aa0f-77c8abe393b9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mbti_data"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = mbti_data[['E/I', 'S/N', 'T/F', 'J/P']]\n",
        "psychological_features = mbti_data.drop(['E/I', 'S/N', 'T/F', 'J/P', 'posts'], axis=1)\n",
        "text = mbti_data['posts']\n",
        "\n",
        "labels.shape, psychological_features.shape, text.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkseIpXIMPJR",
        "outputId": "53067824-3048-48ec-fc3f-a857ad9e5fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8675, 4), (8675, 60), (8675,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(psychological_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "orCOIVYtOSjp",
        "outputId": "95186279-8ed8-435e-a9a4-ee5b2e198504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      Arousal_nwords  Comnobj_GI   Fall_GI  Compassion_GALC   Dist_GI  \\\n",
              "0           0.373525    0.007194  0.003597         0.000000  0.000000   \n",
              "1           0.320863    0.003419  0.000000         0.000000  0.000855   \n",
              "2           0.359031    0.009569  0.000000         0.001196  0.000000   \n",
              "3           0.272979    0.001880  0.000940         0.000000  0.001880   \n",
              "4           0.332802    0.002068  0.000000         0.000000  0.000000   \n",
              "...              ...         ...       ...              ...       ...   \n",
              "8670        0.468492    0.002513  0.000000         0.000000  0.000000   \n",
              "8671        0.311636    0.007645  0.000765         0.000000  0.000000   \n",
              "8672        0.394430    0.004219  0.000000         0.001055  0.000000   \n",
              "8673        0.310370    0.013490  0.001173         0.000587  0.000000   \n",
              "8674        0.374532    0.004422  0.000737         0.000000  0.001474   \n",
              "\n",
              "      Freq_N_OG_FW  Active_GI   Stay_GI    Dim_GI  Contempt_GALC  ...  \\\n",
              "0        10.580120   0.091727  0.005396  0.007194        0.00000  ...   \n",
              "1        10.765889   0.139316  0.001709  0.006838        0.00000  ...   \n",
              "2        10.684414   0.108852  0.003589  0.001196        0.00000  ...   \n",
              "3        10.679634   0.121241  0.000940  0.004699        0.00000  ...   \n",
              "4        10.379383   0.118925  0.002068  0.004137        0.00000  ...   \n",
              "...            ...        ...       ...       ...            ...  ...   \n",
              "8670     10.835137   0.110553  0.000000  0.000000        0.00000  ...   \n",
              "8671     10.911671   0.129205  0.001529  0.002294        0.00000  ...   \n",
              "8672     10.487508   0.117089  0.003165  0.002110        0.00000  ...   \n",
              "8673     11.016429   0.114956  0.002933  0.005279        0.00000  ...   \n",
              "8674     10.905752   0.125276  0.003685  0.004422        0.00147  ...   \n",
              "\n",
              "       Card_GI  Jealousy_GALC   prp_ttr  AWL_Sublist_6_Normed   Time_GI  \\\n",
              "0     0.012590       0.000000  0.240000              0.000000  0.028777   \n",
              "1     0.008547       0.000855  0.097561              0.000805  0.009402   \n",
              "2     0.011962       0.000000  0.122642              0.003448  0.017943   \n",
              "3     0.001880       0.000000  0.119760              0.004505  0.005639   \n",
              "4     0.003102       0.000000  0.188525              0.001970  0.013444   \n",
              "...        ...            ...       ...                   ...       ...   \n",
              "8670  0.005025       0.000000  0.148148              0.000000  0.015075   \n",
              "8671  0.008410       0.000000  0.061947              0.002959  0.010703   \n",
              "8672  0.004219       0.000000  0.131387              0.000000  0.013713   \n",
              "8673  0.004692       0.000587  0.051661              0.001717  0.014663   \n",
              "8674  0.004422       0.000000  0.061404              0.000000  0.015475   \n",
              "\n",
              "      Causal_GI  Boredom_GALC  Joy_GALC  lexical_density_types  eat_types_CW  \n",
              "0      0.017986      0.000000  0.000000               0.779292     40.541502  \n",
              "1      0.019658      0.001709  0.000000               0.809426     40.285714  \n",
              "2      0.026316      0.001196  0.000000               0.790361     39.626478  \n",
              "3      0.031015      0.000000  0.000000               0.777056     41.276087  \n",
              "4      0.019648      0.000000  0.000000               0.787472     40.484144  \n",
              "...         ...           ...       ...                    ...           ...  \n",
              "8670   0.032663      0.000000  0.000000               0.801609     42.087193  \n",
              "8671   0.022936      0.001528  0.000764               0.791753     40.250420  \n",
              "8672   0.018987      0.000000  0.000000               0.779736     39.414414  \n",
              "8673   0.028152      0.000000  0.000000               0.821818     40.010243  \n",
              "8674   0.019160      0.000000  0.000000               0.792035     39.641813  \n",
              "\n",
              "[8675 rows x 60 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd29b672-b166-425d-9214-6acad58c19ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Arousal_nwords</th>\n",
              "      <th>Comnobj_GI</th>\n",
              "      <th>Fall_GI</th>\n",
              "      <th>Compassion_GALC</th>\n",
              "      <th>Dist_GI</th>\n",
              "      <th>Freq_N_OG_FW</th>\n",
              "      <th>Active_GI</th>\n",
              "      <th>Stay_GI</th>\n",
              "      <th>Dim_GI</th>\n",
              "      <th>Contempt_GALC</th>\n",
              "      <th>...</th>\n",
              "      <th>Card_GI</th>\n",
              "      <th>Jealousy_GALC</th>\n",
              "      <th>prp_ttr</th>\n",
              "      <th>AWL_Sublist_6_Normed</th>\n",
              "      <th>Time_GI</th>\n",
              "      <th>Causal_GI</th>\n",
              "      <th>Boredom_GALC</th>\n",
              "      <th>Joy_GALC</th>\n",
              "      <th>lexical_density_types</th>\n",
              "      <th>eat_types_CW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.373525</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.580120</td>\n",
              "      <td>0.091727</td>\n",
              "      <td>0.005396</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028777</td>\n",
              "      <td>0.017986</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.779292</td>\n",
              "      <td>40.541502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.320863</td>\n",
              "      <td>0.003419</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000855</td>\n",
              "      <td>10.765889</td>\n",
              "      <td>0.139316</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>0.006838</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.000855</td>\n",
              "      <td>0.097561</td>\n",
              "      <td>0.000805</td>\n",
              "      <td>0.009402</td>\n",
              "      <td>0.019658</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.809426</td>\n",
              "      <td>40.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.359031</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.684414</td>\n",
              "      <td>0.108852</td>\n",
              "      <td>0.003589</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122642</td>\n",
              "      <td>0.003448</td>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.790361</td>\n",
              "      <td>39.626478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.272979</td>\n",
              "      <td>0.001880</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001880</td>\n",
              "      <td>10.679634</td>\n",
              "      <td>0.121241</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001880</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.119760</td>\n",
              "      <td>0.004505</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.031015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.777056</td>\n",
              "      <td>41.276087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.332802</td>\n",
              "      <td>0.002068</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.379383</td>\n",
              "      <td>0.118925</td>\n",
              "      <td>0.002068</td>\n",
              "      <td>0.004137</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188525</td>\n",
              "      <td>0.001970</td>\n",
              "      <td>0.013444</td>\n",
              "      <td>0.019648</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.787472</td>\n",
              "      <td>40.484144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8670</th>\n",
              "      <td>0.468492</td>\n",
              "      <td>0.002513</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.835137</td>\n",
              "      <td>0.110553</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005025</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.148148</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015075</td>\n",
              "      <td>0.032663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.801609</td>\n",
              "      <td>42.087193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8671</th>\n",
              "      <td>0.311636</td>\n",
              "      <td>0.007645</td>\n",
              "      <td>0.000765</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.911671</td>\n",
              "      <td>0.129205</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.002294</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.061947</td>\n",
              "      <td>0.002959</td>\n",
              "      <td>0.010703</td>\n",
              "      <td>0.022936</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.791753</td>\n",
              "      <td>40.250420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8672</th>\n",
              "      <td>0.394430</td>\n",
              "      <td>0.004219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.487508</td>\n",
              "      <td>0.117089</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.131387</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013713</td>\n",
              "      <td>0.018987</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.779736</td>\n",
              "      <td>39.414414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8673</th>\n",
              "      <td>0.310370</td>\n",
              "      <td>0.013490</td>\n",
              "      <td>0.001173</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.016429</td>\n",
              "      <td>0.114956</td>\n",
              "      <td>0.002933</td>\n",
              "      <td>0.005279</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004692</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.051661</td>\n",
              "      <td>0.001717</td>\n",
              "      <td>0.014663</td>\n",
              "      <td>0.028152</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.821818</td>\n",
              "      <td>40.010243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8674</th>\n",
              "      <td>0.374532</td>\n",
              "      <td>0.004422</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001474</td>\n",
              "      <td>10.905752</td>\n",
              "      <td>0.125276</td>\n",
              "      <td>0.003685</td>\n",
              "      <td>0.004422</td>\n",
              "      <td>0.00147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004422</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.061404</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015475</td>\n",
              "      <td>0.019160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.792035</td>\n",
              "      <td>39.641813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8675 rows × 60 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd29b672-b166-425d-9214-6acad58c19ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cd29b672-b166-425d-9214-6acad58c19ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cd29b672-b166-425d-9214-6acad58c19ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-40d69fc7-abc7-4c94-9275-794495dc3c0d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40d69fc7-abc7-4c94-9275-794495dc3c0d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-40d69fc7-abc7-4c94-9275-794495dc3c0d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "psychological_features"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.optim import Adam\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BertForPsychologicalFeatures(nn.Module):\n",
        "    def __init__(self, bert_model_name, feature_size):\n",
        "        super(BertForPsychologicalFeatures, self).__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name).to(device)\n",
        "        self.dense = nn.Linear(self.bert.config.hidden_size, feature_size).to(device)\n",
        "\n",
        "    def forward(self, texts, attention_mask=None, token_type_ids=None):\n",
        "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
        "        input_ids = inputs['input_ids'].to(device)\n",
        "        attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        feature_output = self.dense(cls_output)\n",
        "\n",
        "        return feature_output\n",
        "\n",
        "def cosine_similarity_loss(cls_vectors, psychological_features):\n",
        "    # Normalizar los vectores\n",
        "    cls_vectors_norm = torch.nn.functional.normalize(cls_vectors, p=2, dim=1)\n",
        "    psychological_features_norm = torch.nn.functional.normalize(psychological_features, p=2, dim=1)\n",
        "\n",
        "    cos_sim = torch.sum(cls_vectors_norm * psychological_features_norm, dim=1)\n",
        "\n",
        "    loss = 1 - cos_sim\n",
        "    return loss.mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "ec6jYntNQENn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PsychologicalFeaturesDataset(Dataset):\n",
        "    def __init__(self, texts, features):\n",
        "        self.texts = texts\n",
        "        self.features = features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        feature = self.features[idx]\n",
        "        return text, feature"
      ],
      "metadata": {
        "id": "7GzBwC8jUeP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_bert(model, data_loader, optimizer):\n",
        "    #Solo entrenas por una epoca, para fintunear bert está bien\n",
        "    model.train()\n",
        "    min_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    for texts, features in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "\n",
        "        loss = cosine_similarity_loss(outputs, features.to(device))\n",
        "\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if loss.item() < min_loss:\n",
        "            min_loss = loss.item()\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "5odgKWtyUa4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "EM-dqkp8bpZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_size = psychological_features.shape[1]\n",
        "model = BertForPsychologicalFeatures('bert-base-uncased', feature_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69coqLYxUPCL",
        "outputId": "52e1afb2-e87a-4e59-e391-39ab7fcaaba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_tensor = torch.tensor(psychological_features.values, dtype=torch.float)\n",
        "\n",
        "\n",
        "dataset = PsychologicalFeaturesDataset(text.values, features_tensor)\n",
        "data_loader = DataLoader(dataset, batch_size=5)"
      ],
      "metadata": {
        "id": "cTKcoC0nSnWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "best_model_bert = train_bert(model, data_loader, optimizer).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz3ITMN3SuAt",
        "outputId": "4777c0ae-0274-4c09-dcee-7dc2c8b7304d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0362707376480103\n",
            "Loss: 0.5327165722846985\n",
            "Loss: 0.20541708171367645\n",
            "Loss: 0.10457602888345718\n",
            "Loss: 0.042741741985082626\n",
            "Loss: 0.036991871893405914\n",
            "Loss: 0.026803279295563698\n",
            "Loss: 0.024042928591370583\n",
            "Loss: 0.02736637555062771\n",
            "Loss: 0.02356419526040554\n",
            "Loss: 0.018958164379000664\n",
            "Loss: 0.017196202650666237\n",
            "Loss: 0.014238977804780006\n",
            "Loss: 0.010537922382354736\n",
            "Loss: 0.012926924042403698\n",
            "Loss: 0.00919187068939209\n",
            "Loss: 0.010894990526139736\n",
            "Loss: 0.00988459587097168\n",
            "Loss: 0.00832741241902113\n",
            "Loss: 0.009674656204879284\n",
            "Loss: 0.008248209953308105\n",
            "Loss: 0.008566773496568203\n",
            "Loss: 0.008435285650193691\n",
            "Loss: 0.008017361164093018\n",
            "Loss: 0.0072177411057055\n",
            "Loss: 0.005730307195335627\n",
            "Loss: 0.008051133714616299\n",
            "Loss: 0.005802083294838667\n",
            "Loss: 0.006260776426643133\n",
            "Loss: 0.007553481962531805\n",
            "Loss: 0.006504094693809748\n",
            "Loss: 0.005689966958016157\n",
            "Loss: 0.005497062113136053\n",
            "Loss: 0.00546653289347887\n",
            "Loss: 0.004610109608620405\n",
            "Loss: 0.005147075746208429\n",
            "Loss: 0.00498355645686388\n",
            "Loss: 0.006289315409958363\n",
            "Loss: 0.005303299520164728\n",
            "Loss: 0.004795503802597523\n",
            "Loss: 0.004887139890342951\n",
            "Loss: 0.004409539978951216\n",
            "Loss: 0.005337536334991455\n",
            "Loss: 0.005679643247276545\n",
            "Loss: 0.0048895361833274364\n",
            "Loss: 0.00403021601960063\n",
            "Loss: 0.004564642906188965\n",
            "Loss: 0.004504287149757147\n",
            "Loss: 0.0044938563369214535\n",
            "Loss: 0.006082999985665083\n",
            "Loss: 0.00441478518769145\n",
            "Loss: 0.004568970296531916\n",
            "Loss: 0.004790747072547674\n",
            "Loss: 0.004150223918259144\n",
            "Loss: 0.004153359215706587\n",
            "Loss: 0.0036256432067602873\n",
            "Loss: 0.004351675510406494\n",
            "Loss: 0.004164707846939564\n",
            "Loss: 0.003900039242580533\n",
            "Loss: 0.004127919673919678\n",
            "Loss: 0.004006576724350452\n",
            "Loss: 0.004362976644188166\n",
            "Loss: 0.004118418786674738\n",
            "Loss: 0.0038558959495276213\n",
            "Loss: 0.003997409250587225\n",
            "Loss: 0.0040069459937512875\n",
            "Loss: 0.003987384028732777\n",
            "Loss: 0.0036959887947887182\n",
            "Loss: 0.0034848214127123356\n",
            "Loss: 0.004861247725784779\n",
            "Loss: 0.003517377423122525\n",
            "Loss: 0.0037626742850989103\n",
            "Loss: 0.004703640937805176\n",
            "Loss: 0.003768360707908869\n",
            "Loss: 0.0036098600830882788\n",
            "Loss: 0.003209698246791959\n",
            "Loss: 0.004836559295654297\n",
            "Loss: 0.003049492835998535\n",
            "Loss: 0.003375446889549494\n",
            "Loss: 0.0034348368644714355\n",
            "Loss: 0.004453134723007679\n",
            "Loss: 0.0035571695771068335\n",
            "Loss: 0.003934753127396107\n",
            "Loss: 0.003179955529049039\n",
            "Loss: 0.0030327916610985994\n",
            "Loss: 0.0047044516541063786\n",
            "Loss: 0.0032289386726915836\n",
            "Loss: 0.0033504010643810034\n",
            "Loss: 0.003149819327518344\n",
            "Loss: 0.0032482983078807592\n",
            "Loss: 0.0034983872901648283\n",
            "Loss: 0.0035064220428466797\n",
            "Loss: 0.0037582756485790014\n",
            "Loss: 0.0034822465386241674\n",
            "Loss: 0.0032269954681396484\n",
            "Loss: 0.003311896463856101\n",
            "Loss: 0.0028559209313243628\n",
            "Loss: 0.004072725772857666\n",
            "Loss: 0.0034956217277795076\n",
            "Loss: 0.0032542587723582983\n",
            "Loss: 0.0031826018821448088\n",
            "Loss: 0.003505146596580744\n",
            "Loss: 0.0037005902267992496\n",
            "Loss: 0.003065216587856412\n",
            "Loss: 0.0031938792672008276\n",
            "Loss: 0.002529394580051303\n",
            "Loss: 0.00314143905416131\n",
            "Loss: 0.0025593042373657227\n",
            "Loss: 0.0035649656783789396\n",
            "Loss: 0.0032482983078807592\n",
            "Loss: 0.002824974013492465\n",
            "Loss: 0.003084218595176935\n",
            "Loss: 0.003780186176300049\n",
            "Loss: 0.0030479549895972013\n",
            "Loss: 0.002742123557254672\n",
            "Loss: 0.0029311419930309057\n",
            "Loss: 0.0027270913124084473\n",
            "Loss: 0.0031536936294287443\n",
            "Loss: 0.0027555704582482576\n",
            "Loss: 0.0033686875831335783\n",
            "Loss: 0.00420534610748291\n",
            "Loss: 0.0031192780006676912\n",
            "Loss: 0.0028885602951049805\n",
            "Loss: 0.0032133578788489103\n",
            "Loss: 0.0026482106186449528\n",
            "Loss: 0.002836609026417136\n",
            "Loss: 0.0027835965156555176\n",
            "Loss: 0.0031536580063402653\n",
            "Loss: 0.0026025057304650545\n",
            "Loss: 0.002694594906643033\n",
            "Loss: 0.002512431237846613\n",
            "Loss: 0.0029976130463182926\n",
            "Loss: 0.0027329325675964355\n",
            "Loss: 0.002834498882293701\n",
            "Loss: 0.0025842548348009586\n",
            "Loss: 0.0023892761673778296\n",
            "Loss: 0.002694416092708707\n",
            "Loss: 0.00297889718785882\n",
            "Loss: 0.003517723176628351\n",
            "Loss: 0.002627897309139371\n",
            "Loss: 0.003277301788330078\n",
            "Loss: 0.0028950453270226717\n",
            "Loss: 0.002319586230441928\n",
            "Loss: 0.0025124549865722656\n",
            "Loss: 0.002594256540760398\n",
            "Loss: 0.0025087713729590178\n",
            "Loss: 0.0025575996842235327\n",
            "Loss: 0.0027624608483165503\n",
            "Loss: 0.002865874907001853\n",
            "Loss: 0.002623617649078369\n",
            "Loss: 0.002993297530338168\n",
            "Loss: 0.0029764175415039062\n",
            "Loss: 0.0026705265045166016\n",
            "Loss: 0.0025286555755883455\n",
            "Loss: 0.004129302687942982\n",
            "Loss: 0.0026755451690405607\n",
            "Loss: 0.002718126866966486\n",
            "Loss: 0.0025557875633239746\n",
            "Loss: 0.0027126313652843237\n",
            "Loss: 0.0023339034523814917\n",
            "Loss: 0.002702605677768588\n",
            "Loss: 0.00261937384493649\n",
            "Loss: 0.0029237032867968082\n",
            "Loss: 0.003202843712642789\n",
            "Loss: 0.0028129934798926115\n",
            "Loss: 0.00227261777035892\n",
            "Loss: 0.0025680901017040014\n",
            "Loss: 0.0024716020561754704\n",
            "Loss: 0.003088998841121793\n",
            "Loss: 0.0025398375000804663\n",
            "Loss: 0.0025983930099755526\n",
            "Loss: 0.0028720141854137182\n",
            "Loss: 0.002805352210998535\n",
            "Loss: 0.0032483458053320646\n",
            "Loss: 0.0027801156975328922\n",
            "Loss: 0.002975594950839877\n",
            "Loss: 0.0030596614815294743\n",
            "Loss: 0.002835166407749057\n",
            "Loss: 0.0026661038864403963\n",
            "Loss: 0.002621471881866455\n",
            "Loss: 0.0025611519813537598\n",
            "Loss: 0.002641189144924283\n",
            "Loss: 0.002064609667286277\n",
            "Loss: 0.0023013234604150057\n",
            "Loss: 0.0027265071403235197\n",
            "Loss: 0.002760589122772217\n",
            "Loss: 0.0026378631591796875\n",
            "Loss: 0.002788758371025324\n",
            "Loss: 0.002205789089202881\n",
            "Loss: 0.002313327742740512\n",
            "Loss: 0.0024866939056664705\n",
            "Loss: 0.0029302954208105803\n",
            "Loss: 0.0028971077408641577\n",
            "Loss: 0.0026369094848632812\n",
            "Loss: 0.002309131668880582\n",
            "Loss: 0.002557945204898715\n",
            "Loss: 0.0024076581466943026\n",
            "Loss: 0.0021777749061584473\n",
            "Loss: 0.0024110793601721525\n",
            "Loss: 0.0027760148514062166\n",
            "Loss: 0.0023363709915429354\n",
            "Loss: 0.0029518248047679663\n",
            "Loss: 0.0024649500846862793\n",
            "Loss: 0.0024596811272203922\n",
            "Loss: 0.0023341060150414705\n",
            "Loss: 0.00282459263689816\n",
            "Loss: 0.0022436976432800293\n",
            "Loss: 0.002520239446312189\n",
            "Loss: 0.0030019402038306\n",
            "Loss: 0.002581000328063965\n",
            "Loss: 0.0029102922417223454\n",
            "Loss: 0.0020512701012194157\n",
            "Loss: 0.002281403634697199\n",
            "Loss: 0.002119863172993064\n",
            "Loss: 0.0023542761337012053\n",
            "Loss: 0.0019975067116320133\n",
            "Loss: 0.002111327601596713\n",
            "Loss: 0.0023044943809509277\n",
            "Loss: 0.0025175809860229492\n",
            "Loss: 0.0022225023712962866\n",
            "Loss: 0.0018167734378948808\n",
            "Loss: 0.0026010393630713224\n",
            "Loss: 0.002406096551567316\n",
            "Loss: 0.0022618533112108707\n",
            "Loss: 0.00275764474645257\n",
            "Loss: 0.002318179700523615\n",
            "Loss: 0.002012741519138217\n",
            "Loss: 0.0021543146576732397\n",
            "Loss: 0.0021554946433752775\n",
            "Loss: 0.0022162317764014006\n",
            "Loss: 0.002274727914482355\n",
            "Loss: 0.0023017884232103825\n",
            "Loss: 0.0018042087322100997\n",
            "Loss: 0.00248315348289907\n",
            "Loss: 0.002306044101715088\n",
            "Loss: 0.002110373927280307\n",
            "Loss: 0.0021893263328820467\n",
            "Loss: 0.0020324350334703922\n",
            "Loss: 0.0022521496284753084\n",
            "Loss: 0.0017674685223028064\n",
            "Loss: 0.0018924117321148515\n",
            "Loss: 0.002157843206077814\n",
            "Loss: 0.002491474151611328\n",
            "Loss: 0.002351272152736783\n",
            "Loss: 0.002234864281490445\n",
            "Loss: 0.002336871577426791\n",
            "Loss: 0.0021844746079295874\n",
            "Loss: 0.002223789691925049\n",
            "Loss: 0.002077484270557761\n",
            "Loss: 0.0019020438194274902\n",
            "Loss: 0.002330303192138672\n",
            "Loss: 0.0018206716049462557\n",
            "Loss: 0.0022586346603929996\n",
            "Loss: 0.0020100236870348454\n",
            "Loss: 0.001812326954677701\n",
            "Loss: 0.0019227027660235763\n",
            "Loss: 0.0022578835487365723\n",
            "Loss: 0.0023254991974681616\n",
            "Loss: 0.0019479513866826892\n",
            "Loss: 0.0018208742840215564\n",
            "Loss: 0.0019174338085576892\n",
            "Loss: 0.001916909241117537\n",
            "Loss: 0.001737403916195035\n",
            "Loss: 0.002303051995113492\n",
            "Loss: 0.001943183015100658\n",
            "Loss: 0.001971077872440219\n",
            "Loss: 0.001755869365297258\n",
            "Loss: 0.002033078810200095\n",
            "Loss: 0.0023790837731212378\n",
            "Loss: 0.0019764781463891268\n",
            "Loss: 0.002039241837337613\n",
            "Loss: 0.0022271634079515934\n",
            "Loss: 0.001960802124813199\n",
            "Loss: 0.0023560167755931616\n",
            "Loss: 0.001965141389518976\n",
            "Loss: 0.002060496946796775\n",
            "Loss: 0.001713573932647705\n",
            "Loss: 0.0022063495125621557\n",
            "Loss: 0.0025983096566051245\n",
            "Loss: 0.0019679665565490723\n",
            "Loss: 0.0019454598659649491\n",
            "Loss: 0.001772201037965715\n",
            "Loss: 0.0019494176376610994\n",
            "Loss: 0.0018567442893981934\n",
            "Loss: 0.002109789988026023\n",
            "Loss: 0.00214291806332767\n",
            "Loss: 0.0019696473609656096\n",
            "Loss: 0.0019288540352135897\n",
            "Loss: 0.0019392967224121094\n",
            "Loss: 0.00204373593442142\n",
            "Loss: 0.002024626825004816\n",
            "Loss: 0.002088356064632535\n",
            "Loss: 0.002098310040310025\n",
            "Loss: 0.0016802191967144608\n",
            "Loss: 0.0018783927662298083\n",
            "Loss: 0.001986229559406638\n",
            "Loss: 0.0018502951133996248\n",
            "Loss: 0.001643919968046248\n",
            "Loss: 0.0016325832111760974\n",
            "Loss: 0.0019249439937993884\n",
            "Loss: 0.0021722435485571623\n",
            "Loss: 0.0017561555141583085\n",
            "Loss: 0.0019367814529687166\n",
            "Loss: 0.0018343449337407947\n",
            "Loss: 0.0014724492793902755\n",
            "Loss: 0.002008032752200961\n",
            "Loss: 0.0020250321831554174\n",
            "Loss: 0.002151715802028775\n",
            "Loss: 0.0019461512565612793\n",
            "Loss: 0.0014547348255291581\n",
            "Loss: 0.0017192602390423417\n",
            "Loss: 0.0021109820809215307\n",
            "Loss: 0.0022267343010753393\n",
            "Loss: 0.0022031664848327637\n",
            "Loss: 0.0019447564845904708\n",
            "Loss: 0.0018969894153997302\n",
            "Loss: 0.0019213318591937423\n",
            "Loss: 0.0016764641040936112\n",
            "Loss: 0.002106928965076804\n",
            "Loss: 0.0019487858517095447\n",
            "Loss: 0.002059459686279297\n",
            "Loss: 0.0021434070076793432\n",
            "Loss: 0.0014374613529071212\n",
            "Loss: 0.0016545773250982165\n",
            "Loss: 0.0017250896198675036\n",
            "Loss: 0.0015975594287738204\n",
            "Loss: 0.0015788913005962968\n",
            "Loss: 0.002267181873321533\n",
            "Loss: 0.0018299699295312166\n",
            "Loss: 0.0018648386467248201\n",
            "Loss: 0.0016803384060040116\n",
            "Loss: 0.0018344521522521973\n",
            "Loss: 0.00192440755199641\n",
            "Loss: 0.0020575763192027807\n",
            "Loss: 0.0017432927852496505\n",
            "Loss: 0.0018083095783367753\n",
            "Loss: 0.0017103791469708085\n",
            "Loss: 0.0017762184143066406\n",
            "Loss: 0.0017491698963567615\n",
            "Loss: 0.0019363164901733398\n",
            "Loss: 0.0020385743118822575\n",
            "Loss: 0.002065300941467285\n",
            "Loss: 0.0014213919639587402\n",
            "Loss: 0.0017979502445086837\n",
            "Loss: 0.0015706897247582674\n",
            "Loss: 0.0015082359313964844\n",
            "Loss: 0.001621890114620328\n",
            "Loss: 0.0017758250469341874\n",
            "Loss: 0.0016073465812951326\n",
            "Loss: 0.0017868041759356856\n",
            "Loss: 0.002093291375786066\n",
            "Loss: 0.001818752265535295\n",
            "Loss: 0.0016458273166790605\n",
            "Loss: 0.0017965913284569979\n",
            "Loss: 0.0020218135323375463\n",
            "Loss: 0.0019813894759863615\n",
            "Loss: 0.0015797853702679276\n",
            "Loss: 0.0018060088623315096\n",
            "Loss: 0.0016247153980657458\n",
            "Loss: 0.0016117453342303634\n",
            "Loss: 0.0017031192546710372\n",
            "Loss: 0.001677441643550992\n",
            "Loss: 0.001853132271207869\n",
            "Loss: 0.0019048452377319336\n",
            "Loss: 0.0016934514278545976\n",
            "Loss: 0.0015387535095214844\n",
            "Loss: 0.0018059492576867342\n",
            "Loss: 0.0018919467693194747\n",
            "Loss: 0.0017043352127075195\n",
            "Loss: 0.0017320513725280762\n",
            "Loss: 0.0014209032524377108\n",
            "Loss: 0.002071809722110629\n",
            "Loss: 0.0016915083397179842\n",
            "Loss: 0.0018103838665410876\n",
            "Loss: 0.0015764356357976794\n",
            "Loss: 0.001503288745880127\n",
            "Loss: 0.001838517258875072\n",
            "Loss: 0.0017902374966070056\n",
            "Loss: 0.0020820260979235172\n",
            "Loss: 0.001808512257412076\n",
            "Loss: 0.0016334176762029529\n",
            "Loss: 0.0016689777839928865\n",
            "Loss: 0.0014418363571166992\n",
            "Loss: 0.0018594503635540605\n",
            "Loss: 0.0019607902504503727\n",
            "Loss: 0.0017037511570379138\n",
            "Loss: 0.0015867591137066483\n",
            "Loss: 0.0013875365257263184\n",
            "Loss: 0.0018179536564275622\n",
            "Loss: 0.0013507365947589278\n",
            "Loss: 0.0015147685771808028\n",
            "Loss: 0.0016357660060748458\n",
            "Loss: 0.001480662846006453\n",
            "Loss: 0.001742804073728621\n",
            "Loss: 0.001659226487390697\n",
            "Loss: 0.0015589356189593673\n",
            "Loss: 0.0016390085220336914\n",
            "Loss: 0.0017134667141363025\n",
            "Loss: 0.0015755653148517013\n",
            "Loss: 0.0017763496143743396\n",
            "Loss: 0.0014075518120080233\n",
            "Loss: 0.0016672611236572266\n",
            "Loss: 0.001556909061037004\n",
            "Loss: 0.0016429544193670154\n",
            "Loss: 0.001535952091217041\n",
            "Loss: 0.0014973164070397615\n",
            "Loss: 0.0014166355831548572\n",
            "Loss: 0.0014992117648944259\n",
            "Loss: 0.0015249252319335938\n",
            "Loss: 0.0016937375767156482\n",
            "Loss: 0.001446545124053955\n",
            "Loss: 0.0013604880077764392\n",
            "Loss: 0.0014412999153137207\n",
            "Loss: 0.0014835238689556718\n",
            "Loss: 0.001517784665338695\n",
            "Loss: 0.0017714500427246094\n",
            "Loss: 0.001574361347593367\n",
            "Loss: 0.0017006874550133944\n",
            "Loss: 0.001597333000972867\n",
            "Loss: 0.0013378382427617908\n",
            "Loss: 0.0015570760006085038\n",
            "Loss: 0.0014996409881860018\n",
            "Loss: 0.0016162156825885177\n",
            "Loss: 0.0016721368301659822\n",
            "Loss: 0.00136737828142941\n",
            "Loss: 0.0013724089367315173\n",
            "Loss: 0.0013692856300622225\n",
            "Loss: 0.0016947627300396562\n",
            "Loss: 0.0015373111236840487\n",
            "Loss: 0.0016297698020935059\n",
            "Loss: 0.0015422701835632324\n",
            "Loss: 0.0013334989780560136\n",
            "Loss: 0.0016160249942913651\n",
            "Loss: 0.0017489076126366854\n",
            "Loss: 0.0015987753868103027\n",
            "Loss: 0.001701188157312572\n",
            "Loss: 0.0014806867111474276\n",
            "Loss: 0.0015741586685180664\n",
            "Loss: 0.0014807224506512284\n",
            "Loss: 0.0017391204601153731\n",
            "Loss: 0.0015168547397479415\n",
            "Loss: 0.0015582084888592362\n",
            "Loss: 0.0014304399956017733\n",
            "Loss: 0.001449847244657576\n",
            "Loss: 0.001227223896421492\n",
            "Loss: 0.0014266729122027755\n",
            "Loss: 0.0015520334709435701\n",
            "Loss: 0.0015098333824425936\n",
            "Loss: 0.0013864397769793868\n",
            "Loss: 0.0014129638439044356\n",
            "Loss: 0.00148173572961241\n",
            "Loss: 0.0016025782097131014\n",
            "Loss: 0.0014914990169927478\n",
            "Loss: 0.0014943123096600175\n",
            "Loss: 0.0016089677810668945\n",
            "Loss: 0.0016672015190124512\n",
            "Loss: 0.0015084504848346114\n",
            "Loss: 0.0016206980217248201\n",
            "Loss: 0.0015105725033208728\n",
            "Loss: 0.0013133048778399825\n",
            "Loss: 0.0016390919918194413\n",
            "Loss: 0.0016500592464581132\n",
            "Loss: 0.0015666127437725663\n",
            "Loss: 0.001431024051271379\n",
            "Loss: 0.0015466570621356368\n",
            "Loss: 0.0016017794841900468\n",
            "Loss: 0.0013975382316857576\n",
            "Loss: 0.0014583588344976306\n",
            "Loss: 0.0015195132000371814\n",
            "Loss: 0.001468920730985701\n",
            "Loss: 0.0011697768932208419\n",
            "Loss: 0.0013235450023785233\n",
            "Loss: 0.0012419819831848145\n",
            "Loss: 0.0013604045379906893\n",
            "Loss: 0.001452243304811418\n",
            "Loss: 0.0014063239796087146\n",
            "Loss: 0.0016013026470318437\n",
            "Loss: 0.0013314842944964767\n",
            "Loss: 0.0013064384693279862\n",
            "Loss: 0.001261127064935863\n",
            "Loss: 0.0017214298713952303\n",
            "Loss: 0.0013852715492248535\n",
            "Loss: 0.0012557387817651033\n",
            "Loss: 0.0012097001308575273\n",
            "Loss: 0.0013729572528973222\n",
            "Loss: 0.0013982414966449142\n",
            "Loss: 0.0015443087322637439\n",
            "Loss: 0.0014308333629742265\n",
            "Loss: 0.00156565906945616\n",
            "Loss: 0.0013887167442589998\n",
            "Loss: 0.00121906993445009\n",
            "Loss: 0.0014298200840130448\n",
            "Loss: 0.0014067530864849687\n",
            "Loss: 0.0015189767582342029\n",
            "Loss: 0.001688277698121965\n",
            "Loss: 0.0014562845462933183\n",
            "Loss: 0.0015325188869610429\n",
            "Loss: 0.0012927294010296464\n",
            "Loss: 0.0012961983447894454\n",
            "Loss: 0.0013370633823797107\n",
            "Loss: 0.0013797521824017167\n",
            "Loss: 0.0015378475654870272\n",
            "Loss: 0.001260519027709961\n",
            "Loss: 0.0012419462436810136\n",
            "Loss: 0.001444697380065918\n",
            "Loss: 0.001208448433317244\n",
            "Loss: 0.0014376997714862227\n",
            "Loss: 0.0013408542145043612\n",
            "Loss: 0.0015417098766192794\n",
            "Loss: 0.0011724353535100818\n",
            "Loss: 0.0015482306480407715\n",
            "Loss: 0.001486063003540039\n",
            "Loss: 0.0011668443912640214\n",
            "Loss: 0.0013510227436199784\n",
            "Loss: 0.0014053344493731856\n",
            "Loss: 0.0013883233768865466\n",
            "Loss: 0.0012331604957580566\n",
            "Loss: 0.001537931035272777\n",
            "Loss: 0.001713657402433455\n",
            "Loss: 0.0013692021602764726\n",
            "Loss: 0.0013126373523846269\n",
            "Loss: 0.0012461781734600663\n",
            "Loss: 0.0015001416904851794\n",
            "Loss: 0.0010443091159686446\n",
            "Loss: 0.0014055132633075118\n",
            "Loss: 0.001423656940460205\n",
            "Loss: 0.0015354752540588379\n",
            "Loss: 0.0013630151515826583\n",
            "Loss: 0.0013813257683068514\n",
            "Loss: 0.0013431430561468005\n",
            "Loss: 0.0012772917980328202\n",
            "Loss: 0.0015396714443340898\n",
            "Loss: 0.0014353275764733553\n",
            "Loss: 0.00140715844463557\n",
            "Loss: 0.0013901711208745837\n",
            "Loss: 0.0015618921024724841\n",
            "Loss: 0.0011463285190984607\n",
            "Loss: 0.001155984471552074\n",
            "Loss: 0.0012541055912151933\n",
            "Loss: 0.001336514949798584\n",
            "Loss: 0.0014865518314763904\n",
            "Loss: 0.0013412118423730135\n",
            "Loss: 0.0012146234512329102\n",
            "Loss: 0.0013676047092303634\n",
            "Loss: 0.0012421965366229415\n",
            "Loss: 0.0014171720249578357\n",
            "Loss: 0.0014698505401611328\n",
            "Loss: 0.0013541221851482987\n",
            "Loss: 0.0013236284721642733\n",
            "Loss: 0.00137586600612849\n",
            "Loss: 0.0013691306812688708\n",
            "Loss: 0.0014246463542804122\n",
            "Loss: 0.0012233853340148926\n",
            "Loss: 0.001408231328241527\n",
            "Loss: 0.0014246344799175858\n",
            "Loss: 0.001621246337890625\n",
            "Loss: 0.0011082648998126388\n",
            "Loss: 0.0012294769985601306\n",
            "Loss: 0.0014770508278161287\n",
            "Loss: 0.0011779547203332186\n",
            "Loss: 0.001329493592493236\n",
            "Loss: 0.0013307452900335193\n",
            "Loss: 0.0011900783283635974\n",
            "Loss: 0.0011274338467046618\n",
            "Loss: 0.0012248397106304765\n",
            "Loss: 0.0014620065921917558\n",
            "Loss: 0.001295971916988492\n",
            "Loss: 0.0013142944080755115\n",
            "Loss: 0.0014874100452288985\n",
            "Loss: 0.0014014721382409334\n",
            "Loss: 0.001345646451227367\n",
            "Loss: 0.0012949586380273104\n",
            "Loss: 0.0014588236808776855\n",
            "Loss: 0.0014124870067462325\n",
            "Loss: 0.0012758851516991854\n",
            "Loss: 0.0013421655166894197\n",
            "Loss: 0.0013753414386883378\n",
            "Loss: 0.0011138797271996737\n",
            "Loss: 0.0012912392849102616\n",
            "Loss: 0.0012059331638738513\n",
            "Loss: 0.0011802673107013106\n",
            "Loss: 0.0011138439876958728\n",
            "Loss: 0.0013965129619464278\n",
            "Loss: 0.0014645338524132967\n",
            "Loss: 0.0014897227520123124\n",
            "Loss: 0.0011124849552288651\n",
            "Loss: 0.0012782335979864001\n",
            "Loss: 0.0011781335342675447\n",
            "Loss: 0.0011887907749041915\n",
            "Loss: 0.001074075698852539\n",
            "Loss: 0.0012777805095538497\n",
            "Loss: 0.0011764407390728593\n",
            "Loss: 0.0013163924450054765\n",
            "Loss: 0.0012953877449035645\n",
            "Loss: 0.0013805509079247713\n",
            "Loss: 0.0013870000839233398\n",
            "Loss: 0.0015764356357976794\n",
            "Loss: 0.0012901545269414783\n",
            "Loss: 0.0012868285411968827\n",
            "Loss: 0.0012236833572387695\n",
            "Loss: 0.001389622688293457\n",
            "Loss: 0.0014192700618878007\n",
            "Loss: 0.0014571667416021228\n",
            "Loss: 0.0011327743995934725\n",
            "Loss: 0.0013168692821636796\n",
            "Loss: 0.0011430502636358142\n",
            "Loss: 0.0012293100589886308\n",
            "Loss: 0.001208961009979248\n",
            "Loss: 0.001470673130825162\n",
            "Loss: 0.0012994647258892655\n",
            "Loss: 0.0013362050522118807\n",
            "Loss: 0.0012374997604638338\n",
            "Loss: 0.000984454178251326\n",
            "Loss: 0.0011829257709905505\n",
            "Loss: 0.0010562777752056718\n",
            "Loss: 0.0012731075985357165\n",
            "Loss: 0.0012400031555444002\n",
            "Loss: 0.0012734771007671952\n",
            "Loss: 0.0013369560474529862\n",
            "Loss: 0.0014685512287542224\n",
            "Loss: 0.001100051449611783\n",
            "Loss: 0.0012373089557513595\n",
            "Loss: 0.0012667536502704024\n",
            "Loss: 0.0013785362243652344\n",
            "Loss: 0.00123853690456599\n",
            "Loss: 0.0011954784858971834\n",
            "Loss: 0.001176083111204207\n",
            "Loss: 0.0011824965476989746\n",
            "Loss: 0.0013191938633099198\n",
            "Loss: 0.0010324836475774646\n",
            "Loss: 0.0010883808135986328\n",
            "Loss: 0.001326489495113492\n",
            "Loss: 0.0013040900230407715\n",
            "Loss: 0.0012452483642846346\n",
            "Loss: 0.0012529969681054354\n",
            "Loss: 0.0010586142307147384\n",
            "Loss: 0.0010738850105553865\n",
            "Loss: 0.00103596446570009\n",
            "Loss: 0.0010705114109441638\n",
            "Loss: 0.0011131524806842208\n",
            "Loss: 0.001278269337490201\n",
            "Loss: 0.0013009667163714767\n",
            "Loss: 0.0013147116405889392\n",
            "Loss: 0.0017157793045043945\n",
            "Loss: 0.0010602474212646484\n",
            "Loss: 0.0010412096744403243\n",
            "Loss: 0.0010604382259771228\n",
            "Loss: 0.0012776375515386462\n",
            "Loss: 0.001308655715547502\n",
            "Loss: 0.0010863662464544177\n",
            "Loss: 0.001135194324888289\n",
            "Loss: 0.0013631224865093827\n",
            "Loss: 0.001213336014188826\n",
            "Loss: 0.0012302518589422107\n",
            "Loss: 0.0014283061027526855\n",
            "Loss: 0.0012259603245183825\n",
            "Loss: 0.0013416766887530684\n",
            "Loss: 0.0013495564926415682\n",
            "Loss: 0.0012479781871661544\n",
            "Loss: 0.0013983369572088122\n",
            "Loss: 0.001024997211061418\n",
            "Loss: 0.0010603547561913729\n",
            "Loss: 0.001005148864351213\n",
            "Loss: 0.0011643528705462813\n",
            "Loss: 0.001149857067503035\n",
            "Loss: 0.001201713108457625\n",
            "Loss: 0.001002240227535367\n",
            "Loss: 0.0010202169651165605\n",
            "Loss: 0.0011180281871929765\n",
            "Loss: 0.001147115253843367\n",
            "Loss: 0.0011635422706604004\n",
            "Loss: 0.0012181521160528064\n",
            "Loss: 0.0011675239074975252\n",
            "Loss: 0.0011489391326904297\n",
            "Loss: 0.001187789486721158\n",
            "Loss: 0.0011633395915850997\n",
            "Loss: 0.0013135791523382068\n",
            "Loss: 0.001157140708528459\n",
            "Loss: 0.0012463092571124434\n",
            "Loss: 0.0013037443859502673\n",
            "Loss: 0.0012632012367248535\n",
            "Loss: 0.0012121916515752673\n",
            "Loss: 0.001195669174194336\n",
            "Loss: 0.0013993143802508712\n",
            "Loss: 0.0011388063430786133\n",
            "Loss: 0.0009087920188903809\n",
            "Loss: 0.0009690999868325889\n",
            "Loss: 0.0010058165062218904\n",
            "Loss: 0.0013602853287011385\n",
            "Loss: 0.001280415104702115\n",
            "Loss: 0.0012281537055969238\n",
            "Loss: 0.0012271404266357422\n",
            "Loss: 0.0011020660167559981\n",
            "Loss: 0.0010941385990008712\n",
            "Loss: 0.0009981751209124923\n",
            "Loss: 0.001130223274230957\n",
            "Loss: 0.0009902119636535645\n",
            "Loss: 0.0010043502552434802\n",
            "Loss: 0.0011517166858538985\n",
            "Loss: 0.0011146068572998047\n",
            "Loss: 0.0010447025997564197\n",
            "Loss: 0.0011287688976153731\n",
            "Loss: 0.0011956215603277087\n",
            "Loss: 0.0013416886795312166\n",
            "Loss: 0.0010273813968524337\n",
            "Loss: 0.0010334134567528963\n",
            "Loss: 0.0011490583419799805\n",
            "Loss: 0.001168107963167131\n",
            "Loss: 0.0010558009380474687\n",
            "Loss: 0.0011795283062383533\n",
            "Loss: 0.0011858344078063965\n",
            "Loss: 0.001389157841913402\n",
            "Loss: 0.001127350376918912\n",
            "Loss: 0.0011930465698242188\n",
            "Loss: 0.0011297703022137284\n",
            "Loss: 0.0014933586353436112\n",
            "Loss: 0.0009261012310162187\n",
            "Loss: 0.001217532204464078\n",
            "Loss: 0.0011097670067101717\n",
            "Loss: 0.0012297987705096602\n",
            "Loss: 0.0013690352207049727\n",
            "Loss: 0.0012969851959496737\n",
            "Loss: 0.001071167062036693\n",
            "Loss: 0.0014698982704430819\n",
            "Loss: 0.0010632396442815661\n",
            "Loss: 0.001088094781152904\n",
            "Loss: 0.0011494040954858065\n",
            "Loss: 0.0010577560169622302\n",
            "Loss: 0.0011857151985168457\n",
            "Loss: 0.0012091040844097733\n",
            "Loss: 0.0009215236059390008\n",
            "Loss: 0.0010886788368225098\n",
            "Loss: 0.001260781311430037\n",
            "Loss: 0.0014857411151751876\n",
            "Loss: 0.0010188579326495528\n",
            "Loss: 0.0012100577587261796\n",
            "Loss: 0.0010423064231872559\n",
            "Loss: 0.0009897112613543868\n",
            "Loss: 0.0013571620220318437\n",
            "Loss: 0.0009600639459677041\n",
            "Loss: 0.0012141108745709062\n",
            "Loss: 0.0011227608192712069\n",
            "Loss: 0.0010941982036456466\n",
            "Loss: 0.0010191559558734298\n",
            "Loss: 0.0012095690472051501\n",
            "Loss: 0.0011691451072692871\n",
            "Loss: 0.0011412501335144043\n",
            "Loss: 0.0011590243084356189\n",
            "Loss: 0.0010389566887170076\n",
            "Loss: 0.0009554863208904862\n",
            "Loss: 0.0010362148750573397\n",
            "Loss: 0.001226019929163158\n",
            "Loss: 0.0010821819305419922\n",
            "Loss: 0.001129281590692699\n",
            "Loss: 0.0009329557651653886\n",
            "Loss: 0.0010343671310693026\n",
            "Loss: 0.001002490520477295\n",
            "Loss: 0.0011322259670123458\n",
            "Loss: 0.001093280385248363\n",
            "Loss: 0.001097905682399869\n",
            "Loss: 0.0010636330116540194\n",
            "Loss: 0.0009941697353497148\n",
            "Loss: 0.0011752486461773515\n",
            "Loss: 0.0012740731472149491\n",
            "Loss: 0.0011216163402423263\n",
            "Loss: 0.0011685967911034822\n",
            "Loss: 0.0010233402717858553\n",
            "Loss: 0.0009509801748208702\n",
            "Loss: 0.0010437369346618652\n",
            "Loss: 0.0011289954418316483\n",
            "Loss: 0.001181483268737793\n",
            "Loss: 0.0012972474796697497\n",
            "Loss: 0.0011781335342675447\n",
            "Loss: 0.0010559558868408203\n",
            "Loss: 0.0011006832355633378\n",
            "Loss: 0.0010646342998370528\n",
            "Loss: 0.0010340571170672774\n",
            "Loss: 0.0009736180654726923\n",
            "Loss: 0.000995075679384172\n",
            "Loss: 0.000979220843873918\n",
            "Loss: 0.0010640263790264726\n",
            "Loss: 0.0010843038326129317\n",
            "Loss: 0.0011265277862548828\n",
            "Loss: 0.0009906173218041658\n",
            "Loss: 0.0011791348224505782\n",
            "Loss: 0.001037383102811873\n",
            "Loss: 0.0010792732937261462\n",
            "Loss: 0.0009590983390808105\n",
            "Loss: 0.0012842536671087146\n",
            "Loss: 0.0011256098514422774\n",
            "Loss: 0.001002454780973494\n",
            "Loss: 0.0010720015270635486\n",
            "Loss: 0.00104600191116333\n",
            "Loss: 0.0009936571586877108\n",
            "Loss: 0.0008921861881390214\n",
            "Loss: 0.0011281848419457674\n",
            "Loss: 0.0010695934761315584\n",
            "Loss: 0.001082909177057445\n",
            "Loss: 0.0011267304653301835\n",
            "Loss: 0.0010405421489849687\n",
            "Loss: 0.0009886861080303788\n",
            "Loss: 0.001138925552368164\n",
            "Loss: 0.0010354042751714587\n",
            "Loss: 0.0010634780628606677\n",
            "Loss: 0.0009936571586877108\n",
            "Loss: 0.0010415792930871248\n",
            "Loss: 0.0011145115131512284\n",
            "Loss: 0.0009655118337832391\n",
            "Loss: 0.0009555220603942871\n",
            "Loss: 0.0010598779190331697\n",
            "Loss: 0.0011612772941589355\n",
            "Loss: 0.0009830594062805176\n",
            "Loss: 0.0010205984581261873\n",
            "Loss: 0.0012309551239013672\n",
            "Loss: 0.0010891795391216874\n",
            "Loss: 0.0010751724475994706\n",
            "Loss: 0.0012497305870056152\n",
            "Loss: 0.0011509180767461658\n",
            "Loss: 0.0009600043413229287\n",
            "Loss: 0.0011724949581548572\n",
            "Loss: 0.0011664510238915682\n",
            "Loss: 0.0008623957983218133\n",
            "Loss: 0.0008766413084231317\n",
            "Loss: 0.001119339489378035\n",
            "Loss: 0.0009425759199075401\n",
            "Loss: 0.0010995864868164062\n",
            "Loss: 0.0011435271007940173\n",
            "Loss: 0.0008934378856793046\n",
            "Loss: 0.0011058807140216231\n",
            "Loss: 0.0010790706146508455\n",
            "Loss: 0.00111569173168391\n",
            "Loss: 0.0011612654197961092\n",
            "Loss: 0.001316416310146451\n",
            "Loss: 0.0012182235950604081\n",
            "Loss: 0.0009019732824526727\n",
            "Loss: 0.0009956598514690995\n",
            "Loss: 0.001111662364564836\n",
            "Loss: 0.0008897065999917686\n",
            "Loss: 0.0010339379077777267\n",
            "Loss: 0.0011506915325298905\n",
            "Loss: 0.0009885072940960526\n",
            "Loss: 0.0011550188064575195\n",
            "Loss: 0.0012929916847497225\n",
            "Loss: 0.0010856747394427657\n",
            "Loss: 0.0009315848583355546\n",
            "Loss: 0.0009822965366765857\n",
            "Loss: 0.0009849787456914783\n",
            "Loss: 0.0007825493812561035\n",
            "Loss: 0.0010354042751714587\n",
            "Loss: 0.0011620760196819901\n",
            "Loss: 0.0009639859199523926\n",
            "Loss: 0.001213121460750699\n",
            "Loss: 0.0010587692959234118\n",
            "Loss: 0.000952088856138289\n",
            "Loss: 0.0010533928871154785\n",
            "Loss: 0.0009700894588604569\n",
            "Loss: 0.0009183883666992188\n",
            "Loss: 0.0008957982063293457\n",
            "Loss: 0.0008718371391296387\n",
            "Loss: 0.0009496688726358116\n",
            "Loss: 0.0010658383835107088\n",
            "Loss: 0.0011958957184106112\n",
            "Loss: 0.0008871436002664268\n",
            "Loss: 0.0009006738546304405\n",
            "Loss: 0.0009633302688598633\n",
            "Loss: 0.0010267019970342517\n",
            "Loss: 0.0009345770231448114\n",
            "Loss: 0.0010279774433001876\n",
            "Loss: 0.0009644031524658203\n",
            "Loss: 0.0009339094394817948\n",
            "Loss: 0.0010051369899883866\n",
            "Loss: 0.0009041786543093622\n",
            "Loss: 0.0009764552232809365\n",
            "Loss: 0.0009591460111550987\n",
            "Loss: 0.0011219263542443514\n",
            "Loss: 0.0010231733322143555\n",
            "Loss: 0.0008531808853149414\n",
            "Loss: 0.0009432077640667558\n",
            "Loss: 0.001184439635835588\n",
            "Loss: 0.0011679291492328048\n",
            "Loss: 0.0011460901005193591\n",
            "Loss: 0.0009197592735290527\n",
            "Loss: 0.0010202288394793868\n",
            "Loss: 0.0012068033684045076\n",
            "Loss: 0.00103245978243649\n",
            "Loss: 0.0009923219913616776\n",
            "Loss: 0.00123168237041682\n",
            "Loss: 0.00094180105952546\n",
            "Loss: 0.0009680867078714073\n",
            "Loss: 0.0011479019885882735\n",
            "Loss: 0.0010144353145733476\n",
            "Loss: 0.0010963917011395097\n",
            "Loss: 0.001039576600305736\n",
            "Loss: 0.0009194851154461503\n",
            "Loss: 0.0008827567216940224\n",
            "Loss: 0.0010977507336065173\n",
            "Loss: 0.0009690523147583008\n",
            "Loss: 0.0012215018505230546\n",
            "Loss: 0.0009324073907919228\n",
            "Loss: 0.0008722305647097528\n",
            "Loss: 0.0009101509931497276\n",
            "Loss: 0.0008346557733602822\n",
            "Loss: 0.0008748412365093827\n",
            "Loss: 0.000939703022595495\n",
            "Loss: 0.0010820627212524414\n",
            "Loss: 0.0008787870756350458\n",
            "Loss: 0.0009935855632647872\n",
            "Loss: 0.0009640813223086298\n",
            "Loss: 0.0010136127239093184\n",
            "Loss: 0.0008317947504110634\n",
            "Loss: 0.0008903503767214715\n",
            "Loss: 0.001086938427761197\n",
            "Loss: 0.0009808778995648026\n",
            "Loss: 0.0008653521654196084\n",
            "Loss: 0.001073646591976285\n",
            "Loss: 0.0008784294477663934\n",
            "Loss: 0.0009880423313006759\n",
            "Loss: 0.0009253621101379395\n",
            "Loss: 0.0008826732519082725\n",
            "Loss: 0.0010048866970464587\n",
            "Loss: 0.0011323451763018966\n",
            "Loss: 0.0009984135394915938\n",
            "Loss: 0.0010543823009356856\n",
            "Loss: 0.00093502999516204\n",
            "Loss: 0.0010314583778381348\n",
            "Loss: 0.0009205699316225946\n",
            "Loss: 0.0010207772720605135\n",
            "Loss: 0.001004934310913086\n",
            "Loss: 0.001045572804287076\n",
            "Loss: 0.0007774472469463944\n",
            "Loss: 0.0008568763732910156\n",
            "Loss: 0.0008700251928530633\n",
            "Loss: 0.0009792924392968416\n",
            "Loss: 0.0008414864423684776\n",
            "Loss: 0.0007878780597820878\n",
            "Loss: 0.0009721756214275956\n",
            "Loss: 0.0009330034372396767\n",
            "Loss: 0.0010080814827233553\n",
            "Loss: 0.0009616613388061523\n",
            "Loss: 0.0008122205617837608\n",
            "Loss: 0.0011093377834185958\n",
            "Loss: 0.0009797572856768966\n",
            "Loss: 0.0009495019912719727\n",
            "Loss: 0.0009261727682314813\n",
            "Loss: 0.0008111358038149774\n",
            "Loss: 0.0009255051845684648\n",
            "Loss: 0.0008595109102316201\n",
            "Loss: 0.0009105801582336426\n",
            "Loss: 0.000881409680005163\n",
            "Loss: 0.000757968460675329\n",
            "Loss: 0.0008833647007122636\n",
            "Loss: 0.0008268714300356805\n",
            "Loss: 0.000997006893157959\n",
            "Loss: 0.0009814382065087557\n",
            "Loss: 0.0009659528732299805\n",
            "Loss: 0.0011413097381591797\n",
            "Loss: 0.0008230924722738564\n",
            "Loss: 0.0009130716207437217\n",
            "Loss: 0.000771558319684118\n",
            "Loss: 0.0008772015571594238\n",
            "Loss: 0.0008202671888284385\n",
            "Loss: 0.001034045242704451\n",
            "Loss: 0.0007363200420513749\n",
            "Loss: 0.0010625243885442615\n",
            "Loss: 0.0008283257484436035\n",
            "Loss: 0.0008736610761843622\n",
            "Loss: 0.0009589314577169716\n",
            "Loss: 0.000869452953338623\n",
            "Loss: 0.001045012497343123\n",
            "Loss: 0.0009418964618816972\n",
            "Loss: 0.000796890293713659\n",
            "Loss: 0.0007835865253582597\n",
            "Loss: 0.0008925199508666992\n",
            "Loss: 0.0008748173713684082\n",
            "Loss: 0.0009434103849343956\n",
            "Loss: 0.0008716463926248252\n",
            "Loss: 0.0009353756904602051\n",
            "Loss: 0.000806403171736747\n",
            "Loss: 0.0008604526519775391\n",
            "Loss: 0.001009523868560791\n",
            "Loss: 0.0008664369815960526\n",
            "Loss: 0.0009358763927593827\n",
            "Loss: 0.0007619738462381065\n",
            "Loss: 0.0008317590109072626\n",
            "Loss: 0.0008293628925457597\n",
            "Loss: 0.0007996917120181024\n",
            "Loss: 0.0007495642057619989\n",
            "Loss: 0.0007760763401165605\n",
            "Loss: 0.0009264707914553583\n",
            "Loss: 0.0007785916677676141\n",
            "Loss: 0.000910258328076452\n",
            "Loss: 0.0008915901416912675\n",
            "Loss: 0.001032626605592668\n",
            "Loss: 0.0007263064617291093\n",
            "Loss: 0.0008201003074645996\n",
            "Loss: 0.0009372115018777549\n",
            "Loss: 0.0009855151874944568\n",
            "Loss: 0.00088162423344329\n",
            "Loss: 0.0008771062130108476\n",
            "Loss: 0.0009241342777386308\n",
            "Loss: 0.0008337378385476768\n",
            "Loss: 0.0009034156682901084\n",
            "Loss: 0.0008708715322427452\n",
            "Loss: 0.0008368611452169716\n",
            "Loss: 0.0008788108825683594\n",
            "Loss: 0.0008345961687155068\n",
            "Loss: 0.0007185578579083085\n",
            "Loss: 0.000859248626511544\n",
            "Loss: 0.0007613659254275262\n",
            "Loss: 0.0007984518888406456\n",
            "Loss: 0.0010910154087468982\n",
            "Loss: 0.0007542014354839921\n",
            "Loss: 0.0006591200944967568\n",
            "Loss: 0.0009255289915017784\n",
            "Loss: 0.0008774519083090127\n",
            "Loss: 0.0008626699564047158\n",
            "Loss: 0.0008919477695599198\n",
            "Loss: 0.0008527279132977128\n",
            "Loss: 0.0008301615598611534\n",
            "Loss: 0.0009147286764346063\n",
            "Loss: 0.000756812107283622\n",
            "Loss: 0.0008592605590820312\n",
            "Loss: 0.0009105801582336426\n",
            "Loss: 0.0009081721655093133\n",
            "Loss: 0.0007687449688091874\n",
            "Loss: 0.0007542610401287675\n",
            "Loss: 0.0008338689804077148\n",
            "Loss: 0.0009105086210183799\n",
            "Loss: 0.0009138346067629755\n",
            "Loss: 0.0009135246509686112\n",
            "Loss: 0.0008010626188479364\n",
            "Loss: 0.0007556438795290887\n",
            "Loss: 0.0008489847532473505\n",
            "Loss: 0.0008284926298074424\n",
            "Loss: 0.0008099555852822959\n",
            "Loss: 0.000701606273651123\n",
            "Loss: 0.0007644534343853593\n",
            "Loss: 0.000854706799145788\n",
            "Loss: 0.0006867289775982499\n",
            "Loss: 0.0008775353780947626\n",
            "Loss: 0.0008772015571594238\n",
            "Loss: 0.0008124947780743241\n",
            "Loss: 0.0007189870229922235\n",
            "Loss: 0.0008310556295327842\n",
            "Loss: 0.0009480714797973633\n",
            "Loss: 0.0008070588228292763\n",
            "Loss: 0.0008156895637512207\n",
            "Loss: 0.0009086966747418046\n",
            "Loss: 0.000870048999786377\n",
            "Loss: 0.000819861888885498\n",
            "Loss: 0.0010321378940716386\n",
            "Loss: 0.0008217454305849969\n",
            "Loss: 0.000620424747467041\n",
            "Loss: 0.0008565664174966514\n",
            "Loss: 0.0008447050931863487\n",
            "Loss: 0.0008104324224404991\n",
            "Loss: 0.0007829904789105058\n",
            "Loss: 0.0009215116733685136\n",
            "Loss: 0.0008397817728109658\n",
            "Loss: 0.0007880806806497276\n",
            "Loss: 0.0009036064147949219\n",
            "Loss: 0.0008566022152081132\n",
            "Loss: 0.0009334087371826172\n",
            "Loss: 0.0007750630611553788\n",
            "Loss: 0.0007339000585488975\n",
            "Loss: 0.000985479331575334\n",
            "Loss: 0.0006832242361269891\n",
            "Loss: 0.0007268667104654014\n",
            "Loss: 0.0007056832546368241\n",
            "Loss: 0.0008643984911032021\n",
            "Loss: 0.0007551431772299111\n",
            "Loss: 0.0008075475925579667\n",
            "Loss: 0.0007945775869302452\n",
            "Loss: 0.0007062435033731163\n",
            "Loss: 0.000692689442075789\n",
            "Loss: 0.0008182406309060752\n",
            "Loss: 0.000779485737439245\n",
            "Loss: 0.0006378889083862305\n",
            "Loss: 0.0007109403959475458\n",
            "Loss: 0.0009349107858724892\n",
            "Loss: 0.0007465720409527421\n",
            "Loss: 0.0007044315570965409\n",
            "Loss: 0.0006914615514688194\n",
            "Loss: 0.0008073568460531533\n",
            "Loss: 0.0008011340978555381\n",
            "Loss: 0.000629985355772078\n",
            "Loss: 0.0006869673961773515\n",
            "Loss: 0.0007264733430929482\n",
            "Loss: 0.000788569450378418\n",
            "Loss: 0.0009279489750042558\n",
            "Loss: 0.0009539246675558388\n",
            "Loss: 0.0007439851760864258\n",
            "Loss: 0.0009080886957235634\n",
            "Loss: 0.000660157238598913\n",
            "Loss: 0.0007700682035647333\n",
            "Loss: 0.0007147431606426835\n",
            "Loss: 0.0007483959197998047\n",
            "Loss: 0.0007405042997561395\n",
            "Loss: 0.0007242202991619706\n",
            "Loss: 0.0005993247032165527\n",
            "Loss: 0.0008570194477215409\n",
            "Loss: 0.0007789134979248047\n",
            "Loss: 0.0006617069593630731\n",
            "Loss: 0.0007287979242391884\n",
            "Loss: 0.0007932305452413857\n",
            "Loss: 0.0007505655521526933\n",
            "Loss: 0.000789856945630163\n",
            "Loss: 0.0008267640951089561\n",
            "Loss: 0.0007710695499554276\n",
            "Loss: 0.0007274270174093544\n",
            "Loss: 0.0007589578744955361\n",
            "Loss: 0.0007916450849734247\n",
            "Loss: 0.0008337378385476768\n",
            "Loss: 0.000710177409928292\n",
            "Loss: 0.0007424831273965538\n",
            "Loss: 0.0007626533624716103\n",
            "Loss: 0.0006769299507141113\n",
            "Loss: 0.0007282138103619218\n",
            "Loss: 0.0008980274433270097\n",
            "Loss: 0.000629639660473913\n",
            "Loss: 0.0006952762487344444\n",
            "Loss: 0.0007742047309875488\n",
            "Loss: 0.0007610201719217002\n",
            "Loss: 0.0009221077198162675\n",
            "Loss: 0.0006985903019085526\n",
            "Loss: 0.0006816863897256553\n",
            "Loss: 0.0008246064535342157\n",
            "Loss: 0.0007117748609744012\n",
            "Loss: 0.0007597566000185907\n",
            "Loss: 0.000786471355240792\n",
            "Loss: 0.0007453560829162598\n",
            "Loss: 0.0007220625993795693\n",
            "Loss: 0.0007940292707644403\n",
            "Loss: 0.0007536530611105263\n",
            "Loss: 0.000895082950592041\n",
            "Loss: 0.000704133533872664\n",
            "Loss: 0.0008343100780621171\n",
            "Loss: 0.000791656959336251\n",
            "Loss: 0.0007172703626565635\n",
            "Loss: 0.0007913828012533486\n",
            "Loss: 0.0007548212888650596\n",
            "Loss: 0.0006764531135559082\n",
            "Loss: 0.0007573485490866005\n",
            "Loss: 0.0007538438076153398\n",
            "Loss: 0.0008745193481445312\n",
            "Loss: 0.0006011128425598145\n",
            "Loss: 0.0008112907526083291\n",
            "Loss: 0.0007619857788085938\n",
            "Loss: 0.0008346677059307694\n",
            "Loss: 0.0007992148748598993\n",
            "Loss: 0.0008590102079324424\n",
            "Loss: 0.0007744550821371377\n",
            "Loss: 0.000623726868070662\n",
            "Loss: 0.0006667494890280068\n",
            "Loss: 0.0007024407386779785\n",
            "Loss: 0.0007957100751809776\n",
            "Loss: 0.0006936431163921952\n",
            "Loss: 0.0009126425138674676\n",
            "Loss: 0.0008100509876385331\n",
            "Loss: 0.0007866263622418046\n",
            "Loss: 0.000786387943662703\n",
            "Loss: 0.0009204507223330438\n",
            "Loss: 0.0007067799451760948\n",
            "Loss: 0.0007450342527590692\n",
            "Loss: 0.0007054329034872353\n",
            "Loss: 0.0007274031522683799\n",
            "Loss: 0.0007753848913125694\n",
            "Loss: 0.000804388546384871\n",
            "Loss: 0.0008557319524697959\n",
            "Loss: 0.0006418466800823808\n",
            "Loss: 0.0008304119110107422\n",
            "Loss: 0.0007340431329794228\n",
            "Loss: 0.0007882476202212274\n",
            "Loss: 0.000815832638181746\n",
            "Loss: 0.0008073807111941278\n",
            "Loss: 0.0007340669981203973\n",
            "Loss: 0.0005002737161703408\n",
            "Loss: 0.0006321787950582802\n",
            "Loss: 0.0007916331524029374\n",
            "Loss: 0.0006625175592489541\n",
            "Loss: 0.0008030891767702997\n",
            "Loss: 0.0006287336582317948\n",
            "Loss: 0.0006639480707235634\n",
            "Loss: 0.0007913589361123741\n",
            "Loss: 0.0007020950433798134\n",
            "Loss: 0.0007456541061401367\n",
            "Loss: 0.0006971835973672569\n",
            "Loss: 0.0007618188974447548\n",
            "Loss: 0.0006355047225952148\n",
            "Loss: 0.0007179498788900673\n",
            "Loss: 0.0006139636388979852\n",
            "Loss: 0.000752055668272078\n",
            "Loss: 0.000595819961745292\n",
            "Loss: 0.0007198095554485917\n",
            "Loss: 0.0007388711092062294\n",
            "Loss: 0.0006149649852886796\n",
            "Loss: 0.0007914781454019248\n",
            "Loss: 0.0005719423643313348\n",
            "Loss: 0.0006490230443887413\n",
            "Loss: 0.000773656356614083\n",
            "Loss: 0.0007439494365826249\n",
            "Loss: 0.0007412314298562706\n",
            "Loss: 0.0007234096410684288\n",
            "Loss: 0.0006334662321023643\n",
            "Loss: 0.0006532192346639931\n",
            "Loss: 0.0006519914022646844\n",
            "Loss: 0.000828778778668493\n",
            "Loss: 0.0008126259199343622\n",
            "Loss: 0.0007079362985678017\n",
            "Loss: 0.0005710005643777549\n",
            "Loss: 0.0006406903266906738\n",
            "Loss: 0.0007913947338238358\n",
            "Loss: 0.0006854772800579667\n",
            "Loss: 0.0008515596273355186\n",
            "Loss: 0.0007254004594869912\n",
            "Loss: 0.0007186532020568848\n",
            "Loss: 0.0008416295167990029\n",
            "Loss: 0.0006746649742126465\n",
            "Loss: 0.0009135365835390985\n",
            "Loss: 0.0006457925192080438\n",
            "Loss: 0.0006043910980224609\n",
            "Loss: 0.0006701469537802041\n",
            "Loss: 0.00074090959969908\n",
            "Loss: 0.0007265090825967491\n",
            "Loss: 0.0006931782118044794\n",
            "Loss: 0.000597965728957206\n",
            "Loss: 0.000663375889416784\n",
            "Loss: 0.0006960392347536981\n",
            "Loss: 0.0007121443632058799\n",
            "Loss: 0.0007148027652874589\n",
            "Loss: 0.0005866169813089073\n",
            "Loss: 0.0007196545484475791\n",
            "Loss: 0.0007793903350830078\n",
            "Loss: 0.0005706906667910516\n",
            "Loss: 0.0006370306364260614\n",
            "Loss: 0.0006853222730569541\n",
            "Loss: 0.0006920814630575478\n",
            "Loss: 0.0007221341365948319\n",
            "Loss: 0.0007900357595644891\n",
            "Loss: 0.0006956100696697831\n",
            "Loss: 0.0007624984136782587\n",
            "Loss: 0.000693333160597831\n",
            "Loss: 0.0005952119827270508\n",
            "Loss: 0.0006700754165649414\n",
            "Loss: 0.0007394432905130088\n",
            "Loss: 0.0006620645872317255\n",
            "Loss: 0.0006355166551657021\n",
            "Loss: 0.0006587267271243036\n",
            "Loss: 0.0007510781288146973\n",
            "Loss: 0.0007867813110351562\n",
            "Loss: 0.000720894371625036\n",
            "Loss: 0.0005425810813903809\n",
            "Loss: 0.0006878137937746942\n",
            "Loss: 0.0006271600723266602\n",
            "Loss: 0.0006967067602090538\n",
            "Loss: 0.0007874131551943719\n",
            "Loss: 0.0007583856931887567\n",
            "Loss: 0.0006869315984658897\n",
            "Loss: 0.0007156252977438271\n",
            "Loss: 0.0006293535116128623\n",
            "Loss: 0.0006604075315408409\n",
            "Loss: 0.0006191015127114952\n",
            "Loss: 0.0006809115293435752\n",
            "Loss: 0.0007429718971252441\n",
            "Loss: 0.0006921649328432977\n",
            "Loss: 0.0006918430444784462\n",
            "Loss: 0.0006701112142764032\n",
            "Loss: 0.0006762146949768066\n",
            "Loss: 0.0006996631855145097\n",
            "Loss: 0.000675761722959578\n",
            "Loss: 0.0006070852396078408\n",
            "Loss: 0.0007737040868960321\n",
            "Loss: 0.0006303549162112176\n",
            "Loss: 0.0006795406225137413\n",
            "Loss: 0.0008484125137329102\n",
            "Loss: 0.000760066497605294\n",
            "Loss: 0.0006251693121157587\n",
            "Loss: 0.0006753564230166376\n",
            "Loss: 0.0006545305368490517\n",
            "Loss: 0.0006319523090496659\n",
            "Loss: 0.0006247162818908691\n",
            "Loss: 0.0008763313526287675\n",
            "Loss: 0.0007393479463644326\n",
            "Loss: 0.0006437063566409051\n",
            "Loss: 0.0006455779075622559\n",
            "Loss: 0.0006346821901388466\n",
            "Loss: 0.0007081151125021279\n",
            "Loss: 0.0007187247392721474\n",
            "Loss: 0.0007397055742330849\n",
            "Loss: 0.0006114959833212197\n",
            "Loss: 0.0007997751235961914\n",
            "Loss: 0.0007300734869204462\n",
            "Loss: 0.0007057547918520868\n",
            "Loss: 0.000736534595489502\n",
            "Loss: 0.0007438898319378495\n",
            "Loss: 0.000671172165311873\n",
            "Loss: 0.0005852699396200478\n",
            "Loss: 0.0007683754083700478\n",
            "Loss: 0.0006257772329263389\n",
            "Loss: 0.0006969451787881553\n",
            "Loss: 0.0006956934812478721\n",
            "Loss: 0.0007490754360333085\n",
            "Loss: 0.0005774259916506708\n",
            "Loss: 0.0005905866855755448\n",
            "Loss: 0.0007093906751833856\n",
            "Loss: 0.0006840467685833573\n",
            "Loss: 0.0005863428232260048\n",
            "Loss: 0.0007391571998596191\n",
            "Loss: 0.0005852938047610223\n",
            "Loss: 0.0006441354635171592\n",
            "Loss: 0.0006549358367919922\n",
            "Loss: 0.0005876660579815507\n",
            "Loss: 0.0006131172413006425\n",
            "Loss: 0.0005697727319784462\n",
            "Loss: 0.0005456805229187012\n",
            "Loss: 0.0006423831218853593\n",
            "Loss: 0.000681245350278914\n",
            "Loss: 0.0006248354911804199\n",
            "Loss: 0.0007145047420635819\n",
            "Loss: 0.0005373954772949219\n",
            "Loss: 0.0006502509349957108\n",
            "Loss: 0.0006423354498110712\n",
            "Loss: 0.0005859375232830644\n",
            "Loss: 0.0007010578992776573\n",
            "Loss: 0.0006045818445272744\n",
            "Loss: 0.000677132629789412\n",
            "Loss: 0.0007463693618774414\n",
            "Loss: 0.0007146716234274209\n",
            "Loss: 0.0006602764478884637\n",
            "Loss: 0.0006088614463806152\n",
            "Loss: 0.000672459602355957\n",
            "Loss: 0.000656557094771415\n",
            "Loss: 0.0006493449327535927\n",
            "Loss: 0.0006119132158346474\n",
            "Loss: 0.0005900383112020791\n",
            "Loss: 0.0007066846010275185\n",
            "Loss: 0.000630366790574044\n",
            "Loss: 0.0006107568624429405\n",
            "Loss: 0.0006232261657714844\n",
            "Loss: 0.0007293462986126542\n",
            "Loss: 0.000651371490675956\n",
            "Loss: 0.0007076025358401239\n",
            "Loss: 0.0006721735117025673\n",
            "Loss: 0.0006295681232586503\n",
            "Loss: 0.0006201505893841386\n",
            "Loss: 0.0006958126905374229\n",
            "Loss: 0.0007467269897460938\n",
            "Loss: 0.0005654573324136436\n",
            "Loss: 0.0007457613828592002\n",
            "Loss: 0.0006476521375589073\n",
            "Loss: 0.0005889058229513466\n",
            "Loss: 0.0006229996797628701\n",
            "Loss: 0.0006718874210491776\n",
            "Loss: 0.0006234645843505859\n",
            "Loss: 0.0006170511478558183\n",
            "Loss: 0.0006642460939474404\n",
            "Loss: 0.0006531476974487305\n",
            "Loss: 0.0005560636636801064\n",
            "Loss: 0.0006822824361734092\n",
            "Loss: 0.000650000583846122\n",
            "Loss: 0.000706017017364502\n",
            "Loss: 0.000605809676926583\n",
            "Loss: 0.0005970001220703125\n",
            "Loss: 0.000627410423476249\n",
            "Loss: 0.0006470799562521279\n",
            "Loss: 0.0006899595609866083\n",
            "Loss: 0.0006114125135354698\n",
            "Loss: 0.0005310416454449296\n",
            "Loss: 0.0006908297655172646\n",
            "Loss: 0.0007078170892782509\n",
            "Loss: 0.0005445003625936806\n",
            "Loss: 0.0009500622982159257\n",
            "Loss: 0.0006232738378457725\n",
            "Loss: 0.0006090879323892295\n",
            "Loss: 0.0005519509431906044\n",
            "Loss: 0.0005894899368286133\n",
            "Loss: 0.0006477236747741699\n",
            "Loss: 0.000647556793410331\n",
            "Loss: 0.0006461620214395225\n",
            "Loss: 0.0007068872801028192\n",
            "Loss: 0.0005792141309939325\n",
            "Loss: 0.0005993962404318154\n",
            "Loss: 0.0005768299452029169\n",
            "Loss: 0.0006620168569497764\n",
            "Loss: 0.0005692958948202431\n",
            "Loss: 0.0006026268238201737\n",
            "Loss: 0.0005977988475933671\n",
            "Loss: 0.0006168484687805176\n",
            "Loss: 0.0005544782034121454\n",
            "Loss: 0.0007054805755615234\n",
            "Loss: 0.0005850434536114335\n",
            "Loss: 0.0005952000501565635\n",
            "Loss: 0.0007519364589825273\n",
            "Loss: 0.0006329893949441612\n",
            "Loss: 0.0005827188724651933\n",
            "Loss: 0.0005596995470114052\n",
            "Loss: 0.0005343795055523515\n",
            "Loss: 0.0006542563787661493\n",
            "Loss: 0.0006584287039004266\n",
            "Loss: 0.0005936145898886025\n",
            "Loss: 0.0004867076931986958\n",
            "Loss: 0.0006607532850466669\n",
            "Loss: 0.0005981684080325067\n",
            "Loss: 0.0006839156267233193\n",
            "Loss: 0.0006728291627950966\n",
            "Loss: 0.0005601048469543457\n",
            "Loss: 0.0007144928094930947\n",
            "Loss: 0.000532042991835624\n",
            "Loss: 0.0006360054248943925\n",
            "Loss: 0.00052390102064237\n",
            "Loss: 0.0005527615430764854\n",
            "Loss: 0.0006802082061767578\n",
            "Loss: 0.0005441904067993164\n",
            "Loss: 0.0005946278688497841\n",
            "Loss: 0.0006363987922668457\n",
            "Loss: 0.0005930424085818231\n",
            "Loss: 0.0005962729337625206\n",
            "Loss: 0.0005786180845461786\n",
            "Loss: 0.0006921649328432977\n",
            "Loss: 0.0004982114187441766\n",
            "Loss: 0.0005760669591836631\n",
            "Loss: 0.0007098675123415887\n",
            "Loss: 0.0005224347114562988\n",
            "Loss: 0.0005665779463015497\n",
            "Loss: 0.00043866634950973094\n",
            "Loss: 0.0005179166910238564\n",
            "Loss: 0.0006743073463439941\n",
            "Loss: 0.0006122231716290116\n",
            "Loss: 0.0006989121320657432\n",
            "Loss: 0.0004959821817465127\n",
            "Loss: 0.0005812525632791221\n",
            "Loss: 0.0006347179296426475\n",
            "Loss: 0.0005680799367837608\n",
            "Loss: 0.0007455825689248741\n",
            "Loss: 0.0006727456930093467\n",
            "Loss: 0.0006287693977355957\n",
            "Loss: 0.0005580067518167198\n",
            "Loss: 0.0004705667670350522\n",
            "Loss: 0.0005804896354675293\n",
            "Loss: 0.0006547332159243524\n",
            "Loss: 0.0005975961685180664\n",
            "Loss: 0.0006437063566409051\n",
            "Loss: 0.000678288924973458\n",
            "Loss: 0.0006696581840515137\n",
            "Loss: 0.0006433248636312783\n",
            "Loss: 0.0005637049907818437\n",
            "Loss: 0.0005894899368286133\n",
            "Loss: 0.0005720377084799111\n",
            "Loss: 0.0005348801496438682\n",
            "Loss: 0.000547945499420166\n",
            "Loss: 0.0006255150074139237\n",
            "Loss: 0.0006578922620974481\n",
            "Loss: 0.0006289362790994346\n",
            "Loss: 0.00061545375501737\n",
            "Loss: 0.0005936026573181152\n",
            "Loss: 0.000449383252998814\n",
            "Loss: 0.0006045103073120117\n",
            "Loss: 0.0005924463621340692\n",
            "Loss: 0.0005960107082501054\n",
            "Loss: 0.0006309747695922852\n",
            "Loss: 0.0005589366191998124\n",
            "Loss: 0.0004948854329995811\n",
            "Loss: 0.0005056142690591514\n",
            "Loss: 0.0005250215763226151\n",
            "Loss: 0.0006137728923931718\n",
            "Loss: 0.0005771279684267938\n",
            "Loss: 0.0005346775287762284\n",
            "Loss: 0.0006620049825869501\n",
            "Loss: 0.0005789757124148309\n",
            "Loss: 0.0006151676061563194\n",
            "Loss: 0.0006486416095867753\n",
            "Loss: 0.0006128668901510537\n",
            "Loss: 0.00046753883361816406\n",
            "Loss: 0.0004749298095703125\n",
            "Loss: 0.000553989433683455\n",
            "Loss: 0.0005865573766641319\n",
            "Loss: 0.0004874587175436318\n",
            "Loss: 0.0005622268072329462\n",
            "Loss: 0.0006633877637796104\n",
            "Loss: 0.0005817890050821006\n",
            "Loss: 0.0006264090770855546\n",
            "Loss: 0.0005245804786682129\n",
            "Loss: 0.0005542755243368447\n",
            "Loss: 0.000548934971448034\n",
            "Loss: 0.000547862087842077\n",
            "Loss: 0.0006339073297567666\n",
            "Loss: 0.0004992604372091591\n",
            "Loss: 0.0005315422895364463\n",
            "Loss: 0.000510096549987793\n",
            "Loss: 0.0005301713827066123\n",
            "Loss: 0.0005466818693093956\n",
            "Loss: 0.0005075335502624512\n",
            "Loss: 0.0005972028011456132\n",
            "Loss: 0.0006151914712972939\n",
            "Loss: 0.00047632455243729055\n",
            "Loss: 0.000557672989089042\n",
            "Loss: 0.0005383491516113281\n",
            "Loss: 0.0004660368140321225\n",
            "Loss: 0.0005334854358807206\n",
            "Loss: 0.000460636627394706\n",
            "Loss: 0.0005641222232952714\n",
            "Loss: 0.0006037831190042198\n",
            "Loss: 0.0005405783886089921\n",
            "Loss: 0.0005817771307192743\n",
            "Loss: 0.00042338372441008687\n",
            "Loss: 0.0005985975149087608\n",
            "Loss: 0.0005755186430178583\n",
            "Loss: 0.0006204605451785028\n",
            "Loss: 0.0005901932599954307\n",
            "Loss: 0.0005834102630615234\n",
            "Loss: 0.0005509257316589355\n",
            "Loss: 0.0005762935033999383\n",
            "Loss: 0.0006482601165771484\n",
            "Loss: 0.000606155430432409\n",
            "Loss: 0.0005216956487856805\n",
            "Loss: 0.00044726135092787445\n",
            "Loss: 0.0005868077278137207\n",
            "Loss: 0.0005344391101971269\n",
            "Loss: 0.0005259871832095087\n",
            "Loss: 0.0005604744073934853\n",
            "Loss: 0.0005110264173708856\n",
            "Loss: 0.0007137656211853027\n",
            "Loss: 0.0004862785281147808\n",
            "Loss: 0.0005704403156414628\n",
            "Loss: 0.0005695104482583702\n",
            "Loss: 0.0005515217781066895\n",
            "Loss: 0.00047576427459716797\n",
            "Loss: 0.000557160412427038\n",
            "Loss: 0.0006392478826455772\n",
            "Loss: 0.000525784504134208\n",
            "Loss: 0.0005398154607973993\n",
            "Loss: 0.00048226118087768555\n",
            "Loss: 0.0005814791074953973\n",
            "Loss: 0.0006407618639059365\n",
            "Loss: 0.0005855440977029502\n",
            "Loss: 0.0004752636014018208\n",
            "Loss: 0.0006693005561828613\n",
            "Loss: 0.0006003022426739335\n",
            "Loss: 0.0005173563840799034\n",
            "Loss: 0.0005180835723876953\n",
            "Loss: 0.0005943298456259072\n",
            "Loss: 0.0005629062652587891\n",
            "Loss: 0.0006806731107644737\n",
            "Loss: 0.000561058521270752\n",
            "Loss: 0.0005105376476421952\n",
            "Loss: 0.0005402207607403398\n",
            "Loss: 0.0005604386678896844\n",
            "Loss: 0.000502598297316581\n",
            "Loss: 0.000561833381652832\n",
            "Loss: 0.0006139397737570107\n",
            "Loss: 0.0005894899368286133\n",
            "Loss: 0.000579106796067208\n",
            "Loss: 0.0004664301814045757\n",
            "Loss: 0.0005667686345987022\n",
            "Loss: 0.0005431055906228721\n",
            "Loss: 0.0005973458173684776\n",
            "Loss: 0.0006498933071270585\n",
            "Loss: 0.0005016803625039756\n",
            "Loss: 0.0005875825881958008\n",
            "Loss: 0.0006799459806643426\n",
            "Loss: 0.0005982876173220575\n",
            "Loss: 0.0006115198484621942\n",
            "Loss: 0.0005666971555911005\n",
            "Loss: 0.0006975650903768837\n",
            "Loss: 0.0005666136858053505\n",
            "Loss: 0.0006366968154907227\n",
            "Loss: 0.0005136370891705155\n",
            "Loss: 0.0006542086484842002\n",
            "Loss: 0.0004936933401040733\n",
            "Loss: 0.0006474971887655556\n",
            "Loss: 0.0006228089332580566\n",
            "Loss: 0.0005357861518859863\n",
            "Loss: 0.0004063844680786133\n",
            "Loss: 0.0005092620849609375\n",
            "Loss: 0.0006141781923361123\n",
            "Loss: 0.0005789279821328819\n",
            "Loss: 0.0005083799478597939\n",
            "Loss: 0.0006596804014407098\n",
            "Loss: 0.0005716443411074579\n",
            "Loss: 0.0004947066190652549\n",
            "Loss: 0.0005976200336590409\n",
            "Loss: 0.0005901098484173417\n",
            "Loss: 0.0006069898954592645\n",
            "Loss: 0.0005390882724896073\n",
            "Loss: 0.000529599201399833\n",
            "Loss: 0.0005464673158712685\n",
            "Loss: 0.00043560267658904195\n",
            "Loss: 0.000569760799407959\n",
            "Loss: 0.0005233764532022178\n",
            "Loss: 0.0006106853834353387\n",
            "Loss: 0.0006524086347781122\n",
            "Loss: 0.0005800843355245888\n",
            "Loss: 0.0005500674596987665\n",
            "Loss: 0.0004810929240193218\n",
            "Loss: 0.0005331635475158691\n",
            "Loss: 0.0005460739484988153\n",
            "Loss: 0.0005209445953369141\n",
            "Loss: 0.0005429387092590332\n",
            "Loss: 0.00044738056021742523\n",
            "Loss: 0.0005614280817098916\n",
            "Loss: 0.0005991458892822266\n",
            "Loss: 0.0005203724140301347\n",
            "Loss: 0.00038335323915816844\n",
            "Loss: 0.0005001187673769891\n",
            "Loss: 0.0005666613578796387\n",
            "Loss: 0.00046340227709151804\n",
            "Loss: 0.0004635214863810688\n",
            "Loss: 0.0004714369715657085\n",
            "Loss: 0.0005623578908853233\n",
            "Loss: 0.00047369004460051656\n",
            "Loss: 0.00047446490498259664\n",
            "Loss: 0.0006090879323892295\n",
            "Loss: 0.0005407691351138055\n",
            "Loss: 0.0005295515293255448\n",
            "Loss: 0.00047276020632125437\n",
            "Loss: 0.0004913449520245194\n",
            "Loss: 0.0005304098012857139\n",
            "Loss: 0.0004745960177388042\n",
            "Loss: 0.00044536590576171875\n",
            "Loss: 0.00046106576337479055\n",
            "Loss: 0.00048170090303756297\n",
            "Loss: 0.00047981739044189453\n",
            "Loss: 0.0004784226475749165\n",
            "Loss: 0.000571393989957869\n",
            "Loss: 0.0005379438516683877\n",
            "Loss: 0.00048600437003187835\n",
            "Loss: 0.00042594672413542867\n",
            "Loss: 0.0005713581922464073\n",
            "Loss: 0.0005976915708743036\n",
            "Loss: 0.0006090998649597168\n",
            "Loss: 0.00044426918611861765\n",
            "Loss: 0.0004946232074871659\n",
            "Loss: 0.00044116974459029734\n",
            "Loss: 0.0005277633899822831\n",
            "Loss: 0.0005327225080691278\n",
            "Loss: 0.0006170153501443565\n",
            "Loss: 0.0005061149713583291\n",
            "Loss: 0.00045586825581267476\n",
            "Loss: 0.0003942251205444336\n",
            "Loss: 0.0004909873241558671\n",
            "Loss: 0.0005448699230328202\n",
            "Loss: 0.0005380630609579384\n",
            "Loss: 0.0004456162569113076\n",
            "Loss: 0.000490212463773787\n",
            "Loss: 0.0004434347210917622\n",
            "Loss: 0.00048438311205245554\n",
            "Loss: 0.0004733562527690083\n",
            "Loss: 0.000623297702986747\n",
            "Loss: 0.0004473566950764507\n",
            "Loss: 0.00043317078961990774\n",
            "Loss: 0.0006063580513000488\n",
            "Loss: 0.00047538281069137156\n",
            "Loss: 0.0004911303403787315\n",
            "Loss: 0.0004690885543823242\n",
            "Loss: 0.0005531072965823114\n",
            "Loss: 0.0005434751510620117\n",
            "Loss: 0.0005404949188232422\n",
            "Loss: 0.00045330525608733296\n",
            "Loss: 0.00041610002517700195\n",
            "Loss: 0.00046770574408583343\n",
            "Loss: 0.0005053162458352745\n",
            "Loss: 0.0005461931577883661\n",
            "Loss: 0.0005496859666891396\n",
            "Loss: 0.0004793047846760601\n",
            "Loss: 0.000526046787854284\n",
            "Loss: 0.0005868077278137207\n",
            "Loss: 0.0005882978439331055\n",
            "Loss: 0.0005331874126568437\n",
            "Loss: 0.0004996896022930741\n",
            "Loss: 0.0005009174346923828\n",
            "Loss: 0.0005025863647460938\n",
            "Loss: 0.0004913091543130577\n",
            "Loss: 0.0004903197404928505\n",
            "Loss: 0.00050439836923033\n",
            "Loss: 0.0004418373282533139\n",
            "Loss: 0.0004464149533305317\n",
            "Loss: 0.0005246639484539628\n",
            "Loss: 0.0004693627415690571\n",
            "Loss: 0.00046825408935546875\n",
            "Loss: 0.00036938191624358296\n",
            "Loss: 0.000542330730240792\n",
            "Loss: 0.0005504012224264443\n",
            "Loss: 0.0004499077913351357\n",
            "Loss: 0.0003868460771627724\n",
            "Loss: 0.0005038619274273515\n",
            "Loss: 0.0005260825273580849\n",
            "Loss: 0.00048508643521927297\n",
            "Loss: 0.000559389591217041\n",
            "Loss: 0.0005456924554891884\n",
            "Loss: 0.0005376935005187988\n",
            "Loss: 0.0006135821458883584\n",
            "Loss: 0.0005386829725466669\n",
            "Loss: 0.0005296826711855829\n",
            "Loss: 0.00046428441419266164\n",
            "Loss: 0.0005490899202413857\n",
            "Loss: 0.0006055831909179688\n",
            "Loss: 0.00045505762682296336\n",
            "Loss: 0.0005364060634747148\n",
            "Loss: 0.0006129980320110917\n",
            "Loss: 0.0005152464145794511\n",
            "Loss: 0.00046365262824110687\n",
            "Loss: 0.0004980206722393632\n",
            "Loss: 0.0005254388088360429\n",
            "Loss: 0.0005214691045694053\n",
            "Loss: 0.000522232090588659\n",
            "Loss: 0.00052642822265625\n",
            "Loss: 0.0005915284273214638\n",
            "Loss: 0.0004411816771607846\n",
            "Loss: 0.0005385041586123407\n",
            "Loss: 0.0005914211506024003\n",
            "Loss: 0.0005262256017886102\n",
            "Loss: 0.0005385756376199424\n",
            "Loss: 0.0004881858767475933\n",
            "Loss: 0.0005581140867434442\n",
            "Loss: 0.00043250323506072164\n",
            "Loss: 0.0004075765609741211\n",
            "Loss: 0.0005566835752688348\n",
            "Loss: 0.0005181789747439325\n",
            "Loss: 0.0004397273005452007\n",
            "Loss: 0.0004461049975361675\n",
            "Loss: 0.0005144596216268837\n",
            "Loss: 0.000489783298689872\n",
            "Loss: 0.0005043864366598427\n",
            "Loss: 0.0005106806638650596\n",
            "Loss: 0.0004785776254720986\n",
            "Loss: 0.00047304629697464406\n",
            "Loss: 0.0004796147404704243\n",
            "Loss: 0.0004721045552287251\n",
            "Loss: 0.00046409369679167867\n",
            "Loss: 0.0005449652671813965\n",
            "Loss: 0.0004462123033590615\n",
            "Loss: 0.000490295875351876\n",
            "Loss: 0.0004883885267190635\n",
            "Loss: 0.00045380592928268015\n",
            "Loss: 0.0004744529724121094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = \"/content/drive/MyDrive/bert_finetune.pth\"\n",
        "# torch.save(best_model_bert.state_dict(), model_path)\n",
        "\n",
        "# best_model_bert = BertForPsychologicalFeatures('bert-base-uncased', feature_size).to(device)\n",
        "\n",
        "# # Cargar el estado del modelo\n",
        "# model.load_state_dict(torch.load(model_path, map_location=device))"
      ],
      "metadata": {
        "id": "Hz6C-qEJJF1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class PsyEncoder(nn.Module):\n",
        "    def __init__(self, feature_dim, nhead, num_encoder_layers, dim_feedforward, output_dim):\n",
        "        super(PsyEncoder, self).__init__()\n",
        "        self.d_model = feature_dim\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=self.d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first= True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = output.mean(dim=1)\n",
        "\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "nd-rx_nqZBPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = psychological_features.shape[1]\n",
        "nhead = 4\n",
        "num_encoder_layers = 2\n",
        "dim_feedforward = 2048\n",
        "output_dim = psychological_features.shape[1]\n",
        "\n",
        "\n",
        "psyEnconder = PsyEncoder(input_dim, nhead, num_encoder_layers, dim_feedforward, output_dim)\n",
        "#Aqui tienes que poner el que has entrenado antes\n",
        "# bert_model = BertForPsychologicalFeatures('bert-base-uncased', feature_size)"
      ],
      "metadata": {
        "id": "scAhqLdPJ_ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class PsyAttention(nn.Module):\n",
        "    def __init__(self, bert_model, psy_encoder_model, num_classes):\n",
        "        super(PsyAttention, self).__init__()\n",
        "        self.bert_for_features = bert_model.to(device)\n",
        "        self.psy_encoder = psy_encoder_model.to(device)\n",
        "\n",
        "        # Congelar los parámetros del modelo BERT\n",
        "        for param in self.bert_for_features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.bert_output_dense = nn.Linear(feature_size, feature_size)\n",
        "        self.psy_output_dense = nn.Linear(feature_size, feature_size)\n",
        "\n",
        "        combined_feature_size = 2*feature_size\n",
        "        self.final_classifier = nn.Linear(combined_feature_size, num_classes)\n",
        "\n",
        "    def forward(self, texts, features):\n",
        "        bert_output = self.bert_for_features(texts)\n",
        "        bert_output = self.bert_output_dense(bert_output).squeeze(1)\n",
        "\n",
        "        psy_output = self.psy_encoder(features.unsqueeze(1))\n",
        "        psy_output = self.psy_output_dense(psy_output)\n",
        "\n",
        "        combined_output = torch.cat((bert_output, psy_output), dim=1)\n",
        "\n",
        "        # Clasificación final\n",
        "        final_output = self.final_classifier(combined_output)\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "skjV62t9ilXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MBTIDataset(Dataset):\n",
        "    def __init__(self, texts, features,labels):\n",
        "        self.texts = texts\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        feature = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return text, feature,label"
      ],
      "metadata": {
        "id": "58xVE4DALnmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_tensor = torch.tensor(psychological_features.values, dtype=torch.float)\n",
        "labels_tensor = torch.tensor(labels.values, dtype=torch.float)\n",
        "\n",
        "\n",
        "dataset = MBTIDataset(text.values, features_tensor,labels_tensor)\n",
        "data_loader = DataLoader(dataset, batch_size=100)\n"
      ],
      "metadata": {
        "id": "_rLSmxEaL6Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psy_attention_model = PsyAttention(best_model_bert, psyEnconder,4).to(device)"
      ],
      "metadata": {
        "id": "SpjLvcgnMJ8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_layers_detailed(model):\n",
        "    total_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            num_params = param.numel()\n",
        "            total_params += num_params\n",
        "            print(f\"{name}, Size: {param.size()}, Params: {num_params}\")\n",
        "    print(f\"Total trainable parameters: {total_params}\")\n",
        "\n",
        "print_trainable_layers_detailed(psy_attention_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tstLL5b3bGei",
        "outputId": "ee4be6af-2ac6-486e-8cc4-8a78c242897e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "psy_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight, Size: torch.Size([180, 60]), Params: 10800\n",
            "psy_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias, Size: torch.Size([180]), Params: 180\n",
            "psy_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight, Size: torch.Size([60, 60]), Params: 3600\n",
            "psy_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.0.linear1.weight, Size: torch.Size([2048, 60]), Params: 122880\n",
            "psy_encoder.transformer_encoder.layers.0.linear1.bias, Size: torch.Size([2048]), Params: 2048\n",
            "psy_encoder.transformer_encoder.layers.0.linear2.weight, Size: torch.Size([60, 2048]), Params: 122880\n",
            "psy_encoder.transformer_encoder.layers.0.linear2.bias, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.0.norm1.weight, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.0.norm1.bias, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.0.norm2.weight, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.0.norm2.bias, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.1.self_attn.in_proj_weight, Size: torch.Size([180, 60]), Params: 10800\n",
            "psy_encoder.transformer_encoder.layers.1.self_attn.in_proj_bias, Size: torch.Size([180]), Params: 180\n",
            "psy_encoder.transformer_encoder.layers.1.self_attn.out_proj.weight, Size: torch.Size([60, 60]), Params: 3600\n",
            "psy_encoder.transformer_encoder.layers.1.self_attn.out_proj.bias, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.1.linear1.weight, Size: torch.Size([2048, 60]), Params: 122880\n",
            "psy_encoder.transformer_encoder.layers.1.linear1.bias, Size: torch.Size([2048]), Params: 2048\n",
            "psy_encoder.transformer_encoder.layers.1.linear2.weight, Size: torch.Size([60, 2048]), Params: 122880\n",
            "psy_encoder.transformer_encoder.layers.1.linear2.bias, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.1.norm1.weight, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.1.norm1.bias, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.1.norm2.weight, Size: torch.Size([60]), Params: 60\n",
            "psy_encoder.transformer_encoder.layers.1.norm2.bias, Size: torch.Size([60]), Params: 60\n",
            "bert_output_dense.weight, Size: torch.Size([60, 60]), Params: 3600\n",
            "bert_output_dense.bias, Size: torch.Size([60]), Params: 60\n",
            "psy_output_dense.weight, Size: torch.Size([60, 60]), Params: 3600\n",
            "psy_output_dense.bias, Size: torch.Size([60]), Params: 60\n",
            "final_classifier.weight, Size: torch.Size([4, 120]), Params: 480\n",
            "final_classifier.bias, Size: torch.Size([4]), Params: 4\n",
            "Total trainable parameters: 533300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# txt, feat, lab = next(iter(data_loader))\n",
        "# otput_prueba = psy_attention_model(txt,feat.to(device) )\n"
      ],
      "metadata": {
        "id": "tRU1rkMMMr7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
        "loss_function = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "b1bB29wkQcwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_model(model, data_loader, optimizer, loss_function, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        predictions, true_labels = [], []\n",
        "        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "        for batch in progress_bar:\n",
        "            txt, feat, lab = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(txt, feat.to(device))\n",
        "            loss = loss_function(outputs, lab.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "            predictions += torch.sigmoid(outputs).round().detach().cpu().numpy().tolist()\n",
        "            true_labels += lab.detach().cpu().numpy().tolist()\n",
        "            progress_bar.set_postfix(loss=(total_loss / (progress_bar.n + 1)))\n",
        "\n",
        "        # Calcular el F1-score\n",
        "        f1 = f1_score(true_labels, predictions, average='samples')\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(data_loader)}, F1-Score: {f1}')\n",
        "\n",
        "train_model(psy_attention_model, data_loader, optimizer, loss_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "0bedd3c7141445e2bfa4d8e48ea9f816",
            "d00b32be718a4089a04fef7edf96a0f3",
            "4a7f3db2e1114145b9ba0c650d04e072",
            "82b05cc6a51d43c581f11f815ef3d707",
            "028e2404741c459bb74d41940c8c212e",
            "b6d4e5bf9a39433fbac08be1484f123d",
            "3ad06ecaa5394a71b015b3482fc3c497",
            "6a82cfb2bd974e258a6d1d3ff26b4937",
            "c02447d17afc43a9b5ae844f6056f9b2",
            "3067040960c24584a2a82922747694f3",
            "360aa61a1392447b9268c1dc6000eed3",
            "bb8978a3b5274404b6638dfeb5c395a2",
            "39300b29da604252b6257c6d7a972af7",
            "cc00e001a3cd4d6e8cd24e40681c54ed",
            "0372e3d75e5d4094a1fd4a941125de23",
            "26169114a1394f66896aa06b2972c1ad",
            "a3affcffc8b64b928c0ddfe0d5475256",
            "0f5f663ee9bb499491342eb602d2e43a",
            "c2444232434d4061ac33e2825e92d361",
            "5c42333cecd94a959a3d425854076cc2",
            "1de6e9db7c864e4fa4cf5403073650f5",
            "0c76ce31924f43f8b73a5d0eab1c70d5"
          ]
        },
        "id": "ou5Kkz3eaVpB",
        "outputId": "6dc6b4cc-236f-45f2-f86d-91247c9c6c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/10:   0%|          | 0/87 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bedd3c7141445e2bfa4d8e48ea9f816"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7419270516812116, F1-Score: 0.3439462055715658\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/10:   0%|          | 0/87 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb8978a3b5274404b6638dfeb5c395a2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eMzNV9NLdu8D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}